<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>A not so brief DNS introduction</title>
      <link href="/2021/A-not-so-brief-DNS-introduction/"/>
      <url>/2021/A-not-so-brief-DNS-introduction/</url>
      
        <content type="html"><![CDATA[<p>The Domain Name System (DNS) is one of the most important pieces of the global internet infrastructure. DNS is an hierarchical and decentralized database system to translate host names into IP addresses. In this post I will share some of my knowledge about DNS.</p><h2 id="Domain-names-and-zones"><a href="#Domain-names-and-zones" class="headerlink" title="Domain names and zones"></a>Domain names and zones</h2><p>To manage a zone in the public internet, we have to buy a domain name like <em>briefbytes.com</em> through a domain registrar. Then we can set the name servers we desire, to manage our DNS records. In my personal case, I’ve chosen Cloudflare for this.</p><p>After having a domain, we can manage the entire zone for that domain. In some companies it is also common to delegate zones. For example, one business unit can manage the root domain and another one could manage all records in the subdomain <em>it.briefbytes.com</em>, using different name servers.</p><p>Most companies will also create their private zones, which are used for internal DNS resolution on <a href="https://tools.ietf.org/html/rfc1918" target="_blank" rel="noopener">private computer networks</a>, with private DNS servers. Two popular DNS servers are Bind9 for Linux and Windows DNS Server for Windows Server. Records can be saved in zone files or any other type of database. Cloud providers, such as Azure, also offer managed DNS solutions like <a href="https://docs.microsoft.com/en-us/azure/dns/private-dns-overview" target="_blank" rel="noopener">Azure Private DNS</a>, which is capable of creating DNS records for machines as they are provisioned, without having configure <a href="https://tools.ietf.org/html/rfc2136" target="_blank" rel="noopener">Dynamic DNS updates</a> using a tool like <em>nsupdate</em>, as is usually done with self-hosted alternatives.</p><h2 id="DNS-records"><a href="#DNS-records" class="headerlink" title="DNS records"></a>DNS records</h2><p>In the DNS protocol, each resource record must have a name, a type, a time to live (TTL) for caching purposes and some data with the format dependant on the type. More details can be found in <a href="https://tools.ietf.org/html/rfc1035" target="_blank" rel="noopener">RFC-1035</a>. The main record types are:</p><ul><li><p>A (IPv4 host address)</p></li><li><p>AAAA (IPv6 host address)</p></li><li><p>CNAME (ALIAS for host address)</p></li><li><p>SOA (Start of Authority)</p></li><li><p>NS (Name Servers)</p></li><li><p>MX (Mail Servers)</p></li><li><p>SRV (Service records for <a href="https://tools.ietf.org/html/rfc2782" target="_blank" rel="noopener">service discovery</a>, however check out <a href="https://www.consul.io/docs/discovery/dns" target="_blank" rel="noopener">Consul</a> and <a href="https://github.com/hashicorp/consul-template" target="_blank" rel="noopener">consul-template</a> they are really awesome for service discovery and work with DNS)</p></li><li><p>TXT (Arbitrary text strings, useful for proving domain ownership)</p></li><li><p>PTR (Reverse lookup, translates IP to name)</p></li></ul><p>There are some peculiarities about different record types. For example, a CNAME record cannot coexist with other records with the same name, which is why it cannot be created in the apex zone (which is often represented by the name ‘@’). TXT records are the only ones that can contain multiple values in a single resource record, although it is more common to create multiple records with a single value. PTR records are part of the reverse lookup zone, not the forward lookup zone like the rest. As such, they cannot be changed with your DNS provider because they are managed by the owners of the IP addresses, so they can only be changed by the respective internet service provider (ISP) or cloud provider, unless it’s your private zone. There’s also wildcard records and glue records.</p><p>When two resource records are created with the same name and type, but different data, they are referred to as resource record sets (<a href="https://tools.ietf.org/html/rfc2181" target="_blank" rel="noopener">RRSet</a>). CNAMEs and SOA are exceptions and cannot exist in sets. RRSets enable round-robin load balancing through DNS by rotating the order in which the list of record values is returned, as the first one is often used by clients. However, this is a poor man’s load balancer because there is no guarantee that the load will be evenly distributed and if a server goes down, the IP will still be cached until the TTL expires. Round-robin DNS is not an alternative to actual L4 or L7 load balancers, which perform health checks and rotation.</p><p>The alternative to RRSets for load balancing at a global level is Anycast. With Anycast, ISPs and cloud providers, using BGP, can publish different routes in different parts of the world to reach certain IPs, making it possible to have multiple machines or network appliances, like load balancers, around the world with the same IP. With Anycast, it is possible to use a solution like <a href="https://docs.microsoft.com/en-us/azure/traffic-manager/traffic-manager-routing-methods" target="_blank" rel="noopener">Azure Traffic Manager</a> to return different DNS responses based on user locations (actually, based on their DNS servers locations, which are also spread across the world using Anycast, so even the single Google DNS IP 8.8.8.8 maps to many servers). If you use <em>dig</em> or <em>nslookup</em> on the Google domain you will get different answers on different countries.</p><h2 id="How-DNS-queries-are-made"><a href="#How-DNS-queries-are-made" class="headerlink" title="How DNS queries are made"></a>How DNS queries are made</h2><p>When trying to resolve a domain like <em>briefbytes.com</em>, the operating system does the following operations to retrieve the associated IP:</p><ol><li><p>Check the hosts file (<em>/etc/hosts</em> on Linux and Mac OS, inside System32 folder on Windows)</p></li><li><p>Check the system DNS cache (both Windows and Linux have a local DNS cache, however Linux does not, although a local caching DNS server like dnsmasq or systemd-resolved can be installed)</p></li><li><p>Send the query to the configured name servers, which by default is set through DHCP (even on Azure virtual networks), but can be changed (<em>/etc/resolv.conf</em> on Linux and Mac OS, network settings on Windows)</p></li></ol><p>When a query is received by a name server, in the case of the Windows DNS server, a widely used DNS server in many enterprises, the following happens:</p><ol><li><p>Check the local cache</p></li><li><p>If the server is authoritative for the zone, return authoritative answer</p></li><li><p>Do a recursive query through a conditional forwarder if a name server is set for a certain domain</p></li><li><p>Do a recursive query through a forwarder if a name server is configured (like 8.8.8.8 for Google DNS or 168.63.129.16 in Azure networks)</p></li><li><p>Do iterative queries on the root name servers and follow referrals, if a forwarder is not set or fails (configurable)</p></li></ol><p>A fully qualified domain names (FQDN) always has a dot at the end to avoid issues with relative names (e.g.: <a href="http://www.briefbytes.com.)" target="_blank" rel="noopener">www.briefbytes.com.)</a>. Here’s one image from the <a href="https://docs.microsoft.com/en-us/windows-server/identity/ad-ds/plan/reviewing-dns-concepts" target="_blank" rel="noopener">Microsoft docs</a>, which explains name resolution very nicely:</p><img src="/2021/A-not-so-brief-DNS-introduction/dns.gif" title="DNS resolution"><p>A recursive query from the root name servers, with the objective of getting an authoritative answer for this blog domain can be done like this:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">nslookup -<span class="built_in">type</span>=ns com a.root-servers.net</span><br><span class="line">nslookup -<span class="built_in">type</span>=ns briefbytes.com a.gtld-servers.net</span><br><span class="line"><span class="comment"># authoritative answer</span></span><br><span class="line">nslookup briefbytes.com ned.ns.cloudflare.com</span><br><span class="line"></span><br><span class="line"><span class="comment"># non authoritative</span></span><br><span class="line">nslookup -debug briefbytes.com 8.8.8.8</span><br><span class="line">  Non-authoritative answer:</span><br><span class="line">  Name:briefbytes.com</span><br><span class="line">  Address: 172.67.165.65</span><br><span class="line">  Name:briefbytes.com</span><br><span class="line">  Address: 104.21.57.175</span><br><span class="line">  Name:briefbytes.com</span><br><span class="line">  Address: 2606:4700:3033::6815:39af</span><br><span class="line">  Name:briefbytes.com</span><br><span class="line">  Address: 2606:4700:3034::ac43:a541</span><br></pre></td></tr></table></figure><p>There are actually 2 name servers configured for my domain in Cloudflare, with some friendly names. Ned is the primary name server and Tani is a secondary name server. Secondary zones are read only copies of the primary (in some cases like Windows DNS server, all domain controllers are actually primary servers, since writes can happen in any server and data is saved to Active Directory and replicated). By doing a SOA query on each name server we can see a serial number. This value is incremented on the primary server when updates are made. The secondary server periodically checks the serial number and does an incremental <a href="https://tools.ietf.org/html/rfc5936" target="_blank" rel="noopener">zone transfer</a> when needed, to sync the records. DNS protocol supports TCP, although UDP is preferred for small requests (up to 512 bytes).</p><h2 id="Security-and-privacy"><a href="#Security-and-privacy" class="headerlink" title="Security and privacy"></a>Security and privacy</h2><p>DNS traffic is not encrypted, so even if you change your default name servers provided by your ISP, they are able to know which sites you visit based on your DNS queries. In my country, ISPs are forced by the government censorship to block certain domain names like LibGen, so changing them is essential for many people. Let’s hope they never decide to block IP address ranges and take down entire regions of the internet.</p><p>To prevent ISPs from snooping our traffic to sell our data, DNS over HTTPS (<a href="https://tools.ietf.org/html/rfc8484" target="_blank" rel="noopener">DoH</a>) seems to be the future. However, keep in mind that the actual DNS servers will still have the data. Browser vendors like <a href="https://blog.mozilla.org/blog/2020/02/25/firefox-continues-push-to-bring-dns-over-https-by-default-for-us-users" target="_blank" rel="noopener">Firefox</a> pushing DoH, however DNS should be a global operating system setting and not set by individual browsers or other applications. I want to set <a href="https://pi-hole.net" target="_blank" rel="noopener">Pi-Hole</a> as a DNS server to block ads on the network and propagate it to all machines via router’s DHCP without additional configuration, this is what standards are for.</p><p>Regarding DNS security there’s also <a href="https://tools.ietf.org/html/rfc4033" target="_blank" rel="noopener">DNSSEC</a> to verify the integrity of DNS records but it is complex and not widely used. To authenticate updates and zone transfers on a DNS database there’s <a href="https://tools.ietf.org/html/rfc2845" target="_blank" rel="noopener">TSIG</a>, which also guarantees message integrity using shared secrets.</p><p>With DNS management comes public key infrastructure (<a href="https://tools.ietf.org/html/rfc2510" target="_blank" rel="noopener">PKI</a>) certificate management, as we want to issue certificates for domain names. On public DNS zones in the internet, we have to pay for the certificates, unless we use <a href="https://letsencrypt.org" target="_blank" rel="noopener">Let’s Encrypt</a>, as they must be signed by a trusted certificate authority (CA). For private DNS zones on our network, we can create a new internal CA and add the root certificate on all the machines we want the issued certificates to be trusted. To avoid managing internal CAs, some companies buy domains for private DNS resolution, instead of using <a href="https://tools.ietf.org/html/rfc8244" target="_blank" rel="noopener">reserved domains</a> like ‘.local’. Then they usually buy wildcard certificates and install them with configuration management tools on the web servers and any other computer should trust these certificates without additional work.</p><p>The DNS protocol is a critical part of any infrastructure. Even the machines that have to be locked down for compliance reasons will often have DNS traffic enabled, which can be used for <a href="https://blogs.akamai.com/2017/09/introduction-to-dns-data-exfiltration.html" target="_blank" rel="noopener">data exfiltration</a>. Using DNS for this is far from ideal, as each outbound lookup query will have a maximum of 255 bytes and order of arrival in unpredictable. From a security perspective, it makes sense to monitor DNS traffic. When exposing a DNS server on the internet, recursive queries should be disabled, so that only authoritative answers are returned.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>This was a not so brief introduction to DNS. One last advice is if you ever need to develop software related with DNS, check the following Go libraries:</p><ul><li><p><a href="https://github.com/miekg/dns" target="_blank" rel="noopener">https://github.com/miekg/dns</a> - to create DNS clients or servers</p></li><li><p><a href="https://github.com/bodgit/tsig" target="_blank" rel="noopener">https://github.com/bodgit/tsig</a> - perform dynamic updates with <a href="https://www.ietf.org/rfc/rfc3645.txt" target="_blank" rel="noopener">GSS-TSIG</a> on Windows Server with ‘secure only’ updates</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> cloud </tag>
            
            <tag> dns </tag>
            
            <tag> infrastructure </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Secure Prometheus with OIDC or LDAP</title>
      <link href="/2020/Secure-Prometheus-with-OIDC-or-LDAP/"/>
      <url>/2020/Secure-Prometheus-with-OIDC-or-LDAP/</url>
      
        <content type="html"><![CDATA[<p>When hosting applications for internal use, it is common to only allow connections from private networks. However, in addition to using a VPN, being able to authenticate and authorize users with a centralized identity system is often required for security reasons. I found two good alternatives that I will describe next, along with a monitoring use case.</p><h2 id="Prometheus-and-Grafana"><a href="#Prometheus-and-Grafana" class="headerlink" title="Prometheus and Grafana"></a>Prometheus and Grafana</h2><p><a href="https://prometheus.io" target="_blank" rel="noopener">Prometheus</a> is one of the most widely used systems for monitoring and alerting, while <a href="https://grafana.com" target="_blank" rel="noopener">Grafana</a> is very popular to create observability dashboards like this one:</p><img src="/2020/Secure-Prometheus-with-OIDC-or-LDAP/grafana.png" title="Grafana dashboard"><p>Although some applications like Grafana already have support for OAuth or LDAP authentication, others like Prometheus delegate this responsibility to other systems.</p><h2 id="Pomerium"><a href="#Pomerium" class="headerlink" title="Pomerium"></a>Pomerium</h2><p><a href="https://www.pomerium.com/docs" target="_blank" rel="noopener">Pomerium</a> is an identity-aware proxy developed in Go. It can be placed in front of internal services and integrate with identity providers using <a href="https://en.wikipedia.org/wiki/OpenID_Connect" target="_blank" rel="noopener">OpenID Connect</a>. Out of the box, it supports identity providers like Google, Github and standard compliant OIDC providers like <a href="https://github.com/panva/node-oidc-provider" target="_blank" rel="noopener">node-oidc-provider</a> or <a href="https://identityserver4.readthedocs.io/en/latest" target="_blank" rel="noopener">IdentityServer</a>. Here’s a comprehensive <a href="https://www.scottbrady91.com/OpenID-Connect/Getting-Started-with-oidc-provider" target="_blank" rel="noopener">guide</a> to setup a provider.</p><h2 id="Authelia"><a href="#Authelia" class="headerlink" title="Authelia"></a>Authelia</h2><p><a href="https://www.authelia.com/docs" target="_blank" rel="noopener">Authelia</a> is an authentication and authorization server written in Go. It requires integration with a reverse proxy, such as Nginx. Instead of OIDC, LDAP is supported so it can be integrated with Active Directory (AD). On my tests I’ve used a local users file and Nginx, as it was the simplest approach. I also had success with AD and HAProxy (with LUA plugins), following the official documentation.</p><h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p>Both auth technologies work quite well. It boils down to what you need to integrate with. Some additional features can be added on the reverse proxy or load balancer, like rate limiting or modifying request headers.</p><p>I’ve setup a <a href="https://github.com/ruial/monitoring-auth-demo" target="_blank" rel="noopener">demo project</a> to demonstrate their capabilities that uses the following architecture:</p><img src="/2020/Secure-Prometheus-with-OIDC-or-LDAP/auth.png" title="Auth flow"><p>Besides proxy mode, which passes all traffic through, Pomerium also supports Forward authentication, which causes the reverse proxy to call an endpoint to verify if the user is authorized. User information is then placed on request headers. Both technologies make use of cookie sessions and redirect the user to the login page when not authenticated. They also support <a href="https://redis.io" target="_blank" rel="noopener">Redis</a>, which is important to preserve sessions during restarts or if using multiple instances.</p>]]></content>
      
      
      
        <tags>
            
            <tag> prometheus </tag>
            
            <tag> ldap </tag>
            
            <tag> oidc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OpenVPN deployment with Terraform and Ansible</title>
      <link href="/2020/OpenVPN-deployment-with-Terraform-and-Ansible/"/>
      <url>/2020/OpenVPN-deployment-with-Terraform-and-Ansible/</url>
      
        <content type="html"><![CDATA[<p>As the number of machines and services that you manage increases, manually creating and configuring your infrastructure is not scalable. As such, provisioning and configuration management tools, such as <a href="https://www.terraform.io" target="_blank" rel="noopener">Terraform</a> and <a href="https://www.ansible.com" target="_blank" rel="noopener">Ansible</a> are extremely useful. I’m going to show how to automate the deployment of <a href="https://openvpn.net" target="_blank" rel="noopener">OpenVPN</a> on <a href="https://azure.microsoft.com" target="_blank" rel="noopener">Azure</a>.</p><h2 id="OpenVPN"><a href="#OpenVPN" class="headerlink" title="OpenVPN"></a>OpenVPN</h2><p>Occasionally, I want to mask my IP address to unlock geo-restricted content. Instead of paying a monthly fee to a VPN provider, a viable alternative is to create a temporary virtual machine on a cloud provider, install OpenVPN, and then destroy it, which is cheap. With automation, this process is very fast. If you want to learn more about OpenVPN and Public Key Infrastructure (PKI), check this <a href="https://www.packtpub.com/product/mastering-openvpn/9781783553136" target="_blank" rel="noopener">book</a>. For production use, there are additional security steps that should be taken, especially on the PKI side.</p><p>You can follow these instructions to deploy your own OpenVPN server:</p><ol><li>Clone the <a href="https://github.com/ruial/openvpn-demo" target="_blank" rel="noopener">GitHub repository</a></li><li>Generate an SSH key (<em>ssh-keygen</em>) or set an admin password for the server</li><li>Create infrastructure using Terraform (check README)</li><li>Edit the inventory.ini or your <em>/etc/hosts</em> to include the server IP</li><li>Run the Ansible playbook (check README)</li><li>Start OpenVPN client (<em>openvpn –config playbooks/out/client-host.conf</em>)</li><li>Destroy infrastructure when done (check README)</li></ol><h2 id="Demo-project-structure"><a href="#Demo-project-structure" class="headerlink" title="Demo project structure"></a>Demo project structure</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">├── playbooks</span><br><span class="line">│   ├── ansible.cfg         - ansible configuration file</span><br><span class="line">│   ├── handlers            - handlers for events</span><br><span class="line">│   ├── inventory.ini       - inventory of servers</span><br><span class="line">│   ├── main.yml            - main playbook (the entrypoint)</span><br><span class="line">│   ├── out                 - output folder for client config</span><br><span class="line">│   ├── tasks</span><br><span class="line">│   │   ├── easyrsa.yml     - easyrsa config tasks</span><br><span class="line">│   │   └── openvpn.yml     - openvpn config tasks</span><br><span class="line">│   └── templates</span><br><span class="line">│       ├── client.conf.j2  - openvpn client config template</span><br><span class="line">│       └── server.conf.j2  - openvpn server config template</span><br><span class="line">├── provision</span><br><span class="line">│   ├── instance.tf         - virtual machine and network association</span><br><span class="line">│   ├── main.tf             - provider definition and resource group</span><br><span class="line">│   ├── network.tf          - virtual network, subnet and security group</span><br><span class="line">│   ├── output.tf           - output variables (IP address)</span><br><span class="line">│   ├── terraform.tfstate   - automatically generated tfstate</span><br><span class="line">│   └── vars.tf             - input variables that can be overridden</span><br><span class="line">└── readme.md               - project info and instructions</span><br></pre></td></tr></table></figure><h2 id="Terraform"><a href="#Terraform" class="headerlink" title="Terraform"></a>Terraform</h2><p>Terraform is the most popular tool to provision resources on public clouds. The infrastructure is defined with a declarative domain-specific-language (DSL) called HashiCorp Configuration Language (HCL). It saves the infrastructure state in a local file, provides locking and allows to plan the changes before applying them. When working with multiple people, <a href="https://www.terraform.io/docs/state/remote.html" target="_blank" rel="noopener">remote state</a> should be used to prevent concurrent runs.</p><p>As each cloud provider is different, there are multiple terraform <a href="https://www.terraform.io/docs/providers/azurerm" target="_blank" rel="noopener">providers</a>. With Terraform we can keep the entire infrastructure as code and others can review changes, which helps reduce human errors. Initial configurations of the operating system (OS) can be specified, which is helpful. Many companies choose to build their own OS image. Here’s a sample of a terraform config file:</p><figure class="highlight python"><figcaption><span>instance.tf</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">resource <span class="string">"azurerm_virtual_machine"</span> <span class="string">"demo-instance"</span> &#123;</span><br><span class="line">  name                  = <span class="string">"$&#123;var.prefix&#125;-vm"</span></span><br><span class="line">  location              = var.location</span><br><span class="line">  resource_group_name   = azurerm_resource_group.demo.name</span><br><span class="line">  network_interface_ids = [azurerm_network_interface.demo-instance.id]</span><br><span class="line">  vm_size               = <span class="string">"Standard_B1ls"</span></span><br><span class="line"></span><br><span class="line">  delete_os_disk_on_termination = true</span><br><span class="line">  delete_data_disks_on_termination = true</span><br><span class="line"></span><br><span class="line">  storage_image_reference &#123;</span><br><span class="line">    publisher = <span class="string">"OpenLogic"</span></span><br><span class="line">    offer     = <span class="string">"CentOS"</span></span><br><span class="line">    sku       = <span class="string">"7.7"</span></span><br><span class="line">    version   = <span class="string">"latest"</span></span><br><span class="line">  &#125;</span><br><span class="line">  storage_os_disk &#123;</span><br><span class="line">    name              = <span class="string">"$&#123;var.prefix&#125;-osdisk1"</span></span><br><span class="line">    caching           = <span class="string">"ReadWrite"</span></span><br><span class="line">    create_option     = <span class="string">"FromImage"</span></span><br><span class="line">    managed_disk_type = <span class="string">"Standard_LRS"</span></span><br><span class="line">  &#125;</span><br><span class="line">  os_profile &#123;</span><br><span class="line">    computer_name  = <span class="string">"demo-instance"</span></span><br><span class="line">    admin_username = <span class="string">"demo"</span></span><br><span class="line">    <span class="comment">#admin_password = "..."</span></span><br><span class="line">  &#125;</span><br><span class="line">  os_profile_linux_config &#123;</span><br><span class="line">    disable_password_authentication = true</span><br><span class="line">    ssh_keys &#123;</span><br><span class="line">      key_data = file(<span class="string">"~/.ssh/id_rsa.pub"</span>)</span><br><span class="line">      path     = <span class="string">"/home/demo/.ssh/authorized_keys"</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h2 id="Ansible"><a href="#Ansible" class="headerlink" title="Ansible"></a>Ansible</h2><p>Ansible is an agentless configuration management tool, which uses SSH to push and execute playbooks. Playbooks contain sequences of tasks and should be idempotent. For reusable code check <a href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_reuse_roles.html" target="_blank" rel="noopener">Ansible Roles</a>. I’ve developed a playbook to install OpenVPN on CentOS:</p><figure class="highlight yaml"><figcaption><span>main.yml</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">- hosts:</span> <span class="string">openvpn</span></span><br><span class="line"><span class="attr">  become:</span> <span class="literal">yes</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  tasks:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">install</span> <span class="string">packages</span></span><br><span class="line"><span class="attr">    package:</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">"<span class="template-variable">&#123;&#123; item &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">      state:</span> <span class="string">present</span></span><br><span class="line"><span class="attr">    loop:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">epel-release</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">firewalld</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">easy-rsa</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">openvpn</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">easy-rsa</span></span><br><span class="line"><span class="attr">    include_tasks:</span> <span class="string">tasks/easyrsa.yml</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">openvpn</span></span><br><span class="line"><span class="attr">    include_tasks:</span> <span class="string">tasks/openvpn.yml</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  handlers:</span></span><br><span class="line"><span class="attr">    - include:</span> <span class="string">handlers/main.yml</span></span><br></pre></td></tr></table></figure><p>I didn’t include all the tasks here because they would take too much space. At first, some required packages are installed and then <em>easy-rsa</em> is used to generate server and client certificates. Then, OpenVPN and the firewall are configured. Lastly, any changes to the server config will trigger a service reload and with the client config automatically transferred, you are able to use your VPN.</p><h2 id="Closing-thoughts"><a href="#Closing-thoughts" class="headerlink" title="Closing thoughts"></a>Closing thoughts</h2><p>There is a lot to say about infrastructure and application deployment. Immutable vs mutable infrastructure, push vs pull configuration management, kubernetes and containers vs virtual machines, cloud managed services vs self hosted, but I leave all these for another day.</p>]]></content>
      
      
      
        <tags>
            
            <tag> automation </tag>
            
            <tag> ansible </tag>
            
            <tag> terraform </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Automation with Node-RED</title>
      <link href="/2020/Automation-with-Node-RED/"/>
      <url>/2020/Automation-with-Node-RED/</url>
      
        <content type="html"><![CDATA[<p>Inspired by <a href="https://apps.apple.com/us/app/shortcuts/id915249334" target="_blank" rel="noopener">iOS Shortcuts</a> and <a href="https://zapier.com" target="_blank" rel="noopener">Zapier</a>, I searched for an open source solution to easily create automations through a web interface and found two projects, <a href="https://nodered.org" target="_blank" rel="noopener">Node-RED</a> and <a href="https://n8n.io" target="_blank" rel="noopener">n8n</a>.</p><h2 id="Node-RED"><a href="#Node-RED" class="headerlink" title="Node-RED"></a>Node-RED</h2><p>As Node-RED is more low-level and mature than n8n, I picked it to create some integrations. It is a flow-based programming tool developed in Node.js and is part of the <a href="https://js.foundation" target="_blank" rel="noopener">JS foundation</a>. From the web browser, applications/flows are created by adding nodes and linking them together to send messages. Custom JavaScript code can be added in function nodes. The flows created are saved as JSON, which allows for easy sharing. To extend Node-RED, new nodes are developed by the community and published <a href="https://flows.nodered.org" target="_blank" rel="noopener">here</a>. I really appreciated their <a href="https://cookbook.nodered.org" target="_blank" rel="noopener">documentation</a> with common use cases. The fastest way to get started is using Docker Compose:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">'3'</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line"><span class="attr">  nodered:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">nodered/node-red:1.1.2</span></span><br><span class="line"><span class="attr">    container_name:</span> <span class="string">nodered</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"1880:1880"</span></span><br><span class="line"><span class="attr">    environment:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">TZ=Europe/Lisbon</span></span><br><span class="line"><span class="attr">    volumes:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">./nodered-data:/data</span></span><br></pre></td></tr></table></figure><p>Node-RED can even be deployed on a Raspberry Pi. If exposing it to the internet make sure to enable authentication and adequate security mechanisms such as a firewall or a reverse proxy. I will briefly describe 2 useful flows created for testing purposes.</p><h2 id="Example-flows"><a href="#Example-flows" class="headerlink" title="Example flows"></a>Example flows</h2><p>The first flow is triggered daily to get the title and description of the <a href="https://www.packtpub.com/packt/offers/free-learning" target="_blank" rel="noopener">Packt</a> daily free e-book through their API. Using the excellent Telegram <a href="https://core.telegram.org/bots" target="_blank" rel="noopener">Bot API</a>, a message is sent to a channel so I never forget to download a nice book.</p><img src="/2020/Automation-with-Node-RED/packt-flow.png" title="Packt flow"><p>The second flow exposes an API with the latest projects in <a href="https://www.producthunt.com" target="_blank" rel="noopener">Product Hunt</a>. Node-RED already has nodes to scrape HTML using selectors and a template engine to render responses, although some data transformations are required to produce the final JSON.</p><img src="/2020/Automation-with-Node-RED/producthunt-flow.png" title="Producthunt flow"><p>The flows were easy to create and topics like logging, using environment variables, web APIs and scraping were straightforward. To test them, import the code available in this <a href="https://gist.github.com/ruial/69dec3c00b991c3b75da16c16907f87d" target="_blank" rel="noopener">gist</a>.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Node-RED is a great tool to create small prototypes and personal automations, however I am still not convinced that low-code visual programming tools are fit for production environments due to edge cases and maintainability concerns.</p>]]></content>
      
      
      
        <tags>
            
            <tag> javascript </tag>
            
            <tag> node.js </tag>
            
            <tag> automation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tips for writing a software engineering dissertation</title>
      <link href="/2020/Tips-for-writing-a-software-engineering-dissertation/"/>
      <url>/2020/Tips-for-writing-a-software-engineering-dissertation/</url>
      
        <content type="html"><![CDATA[<p>During my university journey, I had to write many technical reports or articles about software engineering. I have some tips that helped accelerate the writing of my dissertation and may be helpful for others.</p><h2 id="LaTeX-instead-of-Word"><a href="#LaTeX-instead-of-Word" class="headerlink" title="LaTeX instead of Word"></a>LaTeX instead of Word</h2><p>At first, I thought that LaTeX was too complex and Word was good enough. On big reports, basic tasks like inserting images, references, changing the layout, making sure everything is formatted correctly, etc., always became a hassle with Word. With LaTeX, the initial learning curve is higher, but there are templates for most kinds of documents you’ll ever need, so advanced knowledge is not really required and the output PDF is much less prone to manual errors. If a local setup is not desired, try Overleaf. PowerPoint is fine for presentations, it’s easy, quick and looks good. A possible alternative is reveal.js if you want to use web technologies.</p><h2 id="Version-control"><a href="#Version-control" class="headerlink" title="Version control"></a>Version control</h2><p>Keep your dissertation and all the project code in version control. I prefer using Git and Github to document all the work done. This way I can go back to a previous version at any time and have a backup of everything. Collaboration becomes much easier too. Just avoid including files that can be easily generated at any time (use gitignore), like the dissertation PDF or your compiled software binaries. Take care of the package management solution, all software/libraries versions should be listed. For higher reproducibility, include vendor dependencies and even data sets that are hard to obtain in an automated way. Also keep a README with instructions to compile/run and common debugging steps. Using Docker is highly recommended.</p><h2 id="Zotero-to-manage-references"><a href="#Zotero-to-manage-references" class="headerlink" title="Zotero to manage references"></a>Zotero to manage references</h2><p>Nobody should be managing their references manually, simply use Zotero, which is open-source. With the browser extension, any article, book or webpage can be saved. In most cases, it will correctly extract the metadata. Combined with LaTeX, all I had to do was export the list in BibTex format, select the reference style and the bibliography section was done. Get good references from reputable journals, I like to search articles using ResearchGate and Google Scholar. It is ok to take a look at Wikipedia, but check their references and cite those instead.</p><h2 id="PlantUML-to-generate-UML-diagrams"><a href="#PlantUML-to-generate-UML-diagrams" class="headerlink" title="PlantUML to generate UML diagrams"></a>PlantUML to generate UML diagrams</h2><p>With the open-source PlantUML, diagrams are defined in text, which is ideal to place in version control. In the past, I had to use tools like Visual Paradigm to create UML and BPMN diagrams. Dragging things around, specially when a diagram does not fit very well in the screen, is very time consuming, although the final result can be prettier. I would recommend <a href="https://app.diagrams.net" target="_blank" rel="noopener">diagrams.net</a> as an open-source alternative to PlantUML but only for more advanced use cases.</p><h2 id="Use-your-university-resources"><a href="#Use-your-university-resources" class="headerlink" title="Use your university resources"></a>Use your university resources</h2><p>My university provided various optional lectures on topics such as LaTeX, how to do research, how to manage references and others. In addition, just by being connected to the university network, even through VPN, websites like SpringerLink and ScienceDirect will give free access to resources. There is a lot of research funded by public taxes that is behind closed gates. Keep in touch with your supervisor and schedule frequent meetings. Being a student has many advantages, from free cloud credits with the Github student pack, to the best Jetbrains IDEs.</p><h2 id="Visual-Studio-Code-and-high-quality-extensions"><a href="#Visual-Studio-Code-and-high-quality-extensions" class="headerlink" title="Visual Studio Code and high quality extensions"></a>Visual Studio Code and high quality extensions</h2><p>In my opinion, VS Code is the best text editor. It is open-source, cross-platform, reasonably fast, has integrated terminal, great git support and plenty of high quality extensions. The following extensions are a must for writing:</p><ul><li><a href="https://marketplace.visualstudio.com/items?itemName=James-Yu.latex-workshop" target="_blank" rel="noopener">LaTeX Workshop</a></li><li><a href="https://marketplace.visualstudio.com/items?itemName=jebbs.plantuml" target="_blank" rel="noopener">PlantUML</a></li><li><a href="https://marketplace.visualstudio.com/items?itemName=streetsidesoftware.code-spell-checker" target="_blank" rel="noopener">Code Spell Checker</a></li></ul><h2 id="Other-recommendations"><a href="#Other-recommendations" class="headerlink" title="Other recommendations"></a>Other recommendations</h2><p>Do not leave the writing for last, do it at the same time as the project is developed, otherwise it will be way harder to stay motivated. Ask other people to read your dissertation, they will likely find flaws you haven’t identified before. When doing a project in a company, seek monetary compensation, if they are cheap now, they will be cheap later. Experiment different tools and workflows to find what works best for you.</p>]]></content>
      
      
      
        <tags>
            
            <tag> latex </tag>
            
            <tag> uml </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Avoid web scraping detection</title>
      <link href="/2020/Avoid-web-scraping-detection/"/>
      <url>/2020/Avoid-web-scraping-detection/</url>
      
        <content type="html"><![CDATA[<p>Web scraping is the process of extracting useful data from a website. Google built their company on this. Many websites take measures to prevent scraping for various reasons, while still allowing big search engines to crawl their content. I will share my opinion about this subject and how some websites detect web scraping.</p><h2 id="Motivation-for-web-scraping"><a href="#Motivation-for-web-scraping" class="headerlink" title="Motivation for web scraping"></a>Motivation for web scraping</h2><p>There are many legitimate reasons to scrape a website and I have a short story of mine. A few years ago, I really enjoyed watching memes from 9gag during my long train commute to university. Sadly, I had very low mobile data, so I created an automation to scrape a few pages of memes during the night and transfer them to my phone so I could watch them offline. I inspected their internal API and created a javascript client, which I open-sourced and shared with the community on <a href="https://www.npmjs.com/package/9gag" target="_blank" rel="noopener">NPM</a>. This small contribution is now a part of fun personal projects made by other people like a <a href="https://github.com/casassg/meme_puller" target="_blank" rel="noopener">machine learning classifier</a>, a <a href="https://github.com/NebulaEngineering/lab-cqrs-es" target="_blank" rel="noopener">CQRS and event sourcing lab</a>, a <a href="https://github.com/Sam-Ryan/Scylla" target="_blank" rel="noopener">discord bot</a>, an <a href="https://github.com/danylkaaa/ReservoirCodeInt20Test" target="_blank" rel="noopener">aggregator website</a> and others.</p><p>Scraping enables automation, giving more freedom to power users. Some platforms do not like this because we won’t see their ads, however most power users block ads anyway as they usually cripple web apps. Other reasons to scrape may not be very ethical. For example, there are many bots scraping the web in search for emails to send spam. Any website is free to block scraping in any way they want, after all they are the ones that choose how to reply to requests. Linkedin sued a company for scraping their public content, but the court did not give them reason. A website term of services page hidden somewhere with a lot of text that no one reads is not really legally enforceable.</p><p>If websites provide a reasonable API, web scraping can be reduced. This way they can enforce limits on API clients, like the ammount of requests allowed per day, track how it is used and even make money by charging for additional API calls. Most web applications are already using internal APIs, they could be made public. Frequently, scrapers actually consume these internal APIs instead of parsing the whole website, which is more efficient for both sides.</p><h2 id="How-web-scraping-works-and-evade-detection"><a href="#How-web-scraping-works-and-evade-detection" class="headerlink" title="How web scraping works and evade detection"></a>How web scraping works and evade detection</h2><p>A web application can be rendered on the server side or client side. When the content rendered on the server and sent to the browser, scraping is easier because only HTML has to be parsed. When rendering happens on client side, a javascript interpreter is required to render the content, so a simple HTTP client is not enough to scrape content, a browser emulator like <a href="https://github.com/puppeteer/puppeteer" target="_blank" rel="noopener">Puppeteer</a> must be used, which is more resource intensive. I recently had to update my old library to use Puppeteer to avoid a CAPTCHA check.</p><p>It is ironic that Google created their empire on web scraping and are the ones to create solutions like CAPTCHA to prevent others from crawling. Some websites even include metadata to help Google scrap their content more accurately and easily, using <a href="https://json-ld.org" target="_blank" rel="noopener">JSON-LD</a> for SEO purposes. There are many other crawlers that behave nicely and have good intentions.</p><p>So how is web scraping detected? Common methods include checking HTTP headers like the user agent, device fingerprinting with javascript and inspecting connection or behaviour patterns. The easiest solution to avoid being detected is to use Puppeteer with a <a href="https://www.npmjs.com/package/puppeteer-extra-plugin-stealth" target="_blank" rel="noopener">stealth extension</a>, which already takes some steps to avoid detection. In addition, navigation should look like a real user, with random time between requests and do not access URLs not visible to a user. A pool of servers with different IP addresses may be needed for mass scale crawling, however many companies can detect if the IP is from major cloud providers, so residential IPs are better.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Web scraping detection will always be a cat and mouse game and some ways of detecting this were presented. However, in the end, if information is publicly available, even a real human can act as a web scraper with or without web extensions to help automate the process.</p>]]></content>
      
      
      
        <tags>
            
            <tag> scraping </tag>
            
            <tag> javascript </tag>
            
            <tag> node.js </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Machine learning basics</title>
      <link href="/2020/Machine-learning-basics/"/>
      <url>/2020/Machine-learning-basics/</url>
      
        <content type="html"><![CDATA[<p>In this post I will briefly describe how to achieve a good score for the Titanic survival classification, the house prices regression, and the NLP twitter disaster problems in Kaggle. The <a href="https://topepo.github.io/caret/available-models.html" target="_blank" rel="noopener">caret</a> library in R makes it easy to test different machine learning algorithms.</p><h2 id="Machine-learning"><a href="#Machine-learning" class="headerlink" title="Machine learning"></a>Machine learning</h2><p>Machine learning algorithms are able to learn from past data how to make predictions on new data. The most common types of ML algorithms are supervised or unsupervised.</p><p>Supervised algorithms are used to solve classification (predict a class) and regression (predict continuous variable) problems. Training data contains labels and common metrics to evaluate models are accuracy, precision and recall for classification, or root mean square error (RMSE) and R-squared (R2) for regression. Prediction of time series can be made using other type of statistical models, like ARIMA and <a href="https://facebook.github.io/prophet" target="_blank" rel="noopener">prophet</a>.</p><p>Unsupervised algorithms usually consist on dimensionality reduction (PCA, SVD, t-sne), clustering (hierarchical, k-means, DBSCAN), anomaly detection and a few others. These do not require training data, and there are different techniques to evaluate each algorithm.</p><h2 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h2><p>The Titanic problem is a binary classification problem that consists on predicting the passengers who survived the disaster. Let’s take a look at the data.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt; glimpse(titanic_train)</span><br><span class="line">Observations: 891</span><br><span class="line">Variables: 12</span><br><span class="line">$ PassengerId &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 2...</span><br><span class="line">$ Survived    &lt;int&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, ...</span><br><span class="line">$ Pclass      &lt;int&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, ...</span><br><span class="line">$ Name        &lt;fct&gt; &quot;Braund, Mr. Owen Harris&quot;, &quot;Cumings, Mrs. John Bradley (Florence Briggs Thayer)&quot;, &quot;Heikkinen, Mi...</span><br><span class="line">$ Sex         &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male...</span><br><span class="line">$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, 55, 2, NA, 31, NA, 35, 34, 15, 28, 8, ...</span><br><span class="line">$ SibSp       &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0, 1, ...</span><br><span class="line">$ Parch       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 2, 0, 0, 0, 0, ...</span><br><span class="line">$ Ticket      &lt;fct&gt; A/5 21171, PC 17599, STON/O2. 3101282, 113803, 373450, 330877, 17463, 349909, 347742, 237736, PP...</span><br><span class="line">$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 2...</span><br><span class="line">$ Cabin       &lt;fct&gt; , C85, , C123, , , E46, , , , G6, C103, , , , , , , , , , D56, , A6, , , , C23 C25 C27, , , , B7...</span><br><span class="line">$ Embarked    &lt;fct&gt; S, C, S, S, S, Q, S, S, S, C, S, S, S, S, S, S, Q, S, S, C, S, S, Q, S, S, S, C, S, Q, S, C, C, ...</span><br></pre></td></tr></table></figure><p>Feature engineering can be more important than the machine learning algorithms applied. Some features like Cabin contain too many missing values and should be dropped. Other features can be added, like the Title (Mr, Ms) from the name, or the Fsize (family size) taking into account the SibSp (siblings/spouses), Parch (parents/children) and Ticket (number of duplicate tickets). Any missing values in the observations must be imputed.</p><p>As a baseline, since most people died, a naive algorithm could predict that all people died and would achieve 62% accuracy on the train set. A smarter baseline model would predict that all men died and women survived, which would result in 79% accuracy. A machine learning algorithm should recognize patterns in the data and have better accuracy. Cross-validation (CV) is useful to prevent models from overfitting (achieving high train accuracy but low test accuracy).</p><p>I’ve tried logistic regression, random forest, gradient boosting, however the algorithm that achieved best results in Kaggle was a simple tree classifier with 83% CV accuracy and 80% accuracy in the test set. The smart baseline would result in 77% test accuracy, so a small improvement was made.</p><img src="/2020/Machine-learning-basics/tree.png" title="Decision tree"><p>With this decision tree, the Title, Pclass, Fsize, Sex and Embarked are the features used to decide the fate of a passenger. A simple visualization of the train data shows hints on why this tree was made.</p><img src="/2020/Machine-learning-basics/titanic-data.png" title="Titanic data"><p>The tree is very interpretable. It captures that on Pclass 3, big families die, as well as Misses who embarked on S. For Pclass 1 and 2 it predicts that children and women live.</p><h2 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a>Regression</h2><p>For this problem the predicted variable, Price, is skewed to the right, so a log transformation is helpful to make the distribution more normal and improve the predictions. The train dataset contains 1460 observations and 81 variables, most of which are useless.</p><p>A lot of feature engineering was done to improve the predictions. A few variables have missing values but can be imputed with related variables. Right skewed variables should be transformed using a log function. Encoding a few categorical values as ordinals and removing outliers also improves results. A correlation matrix can be helpful to select the most important features for models that are sensitive to multicollinearity, like linear regression.</p><img src="/2020/Machine-learning-basics/correlation.png" title="Correlation matrix"><p>Lasso regression achieved 0.11183 CV RMSE and 0.11911 test RMSE. Elasticnet was close behind, while ridge/linear regression and gradient boosting were worst but still an improvement over a mean baseline.</p><p>To improve results I used k-means clustering to create 2 clusters and train a lasso model for each cluster. With this approach, the cluster for each test observation had to be predicted. Performance improved to 0.11897 RMSE. I have seen this cluster and then predict approach in a MOOC I have completed on EDX called The Analytics Edge, which I do recommend as it was very practical, easy to follow, covered many topics and included many examples and exercises.</p><img src="/2020/Machine-learning-basics/clusters.png" title="Clusters"><p>With a weighted combination of all models, a final 0.11765 RMSE was reached, which at the time was enough to be in the top 20%. To further improve results more feature engineering and hyperparameter tuning would be required.</p><h2 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h2><p>For this natural language problem I have used the R <a href="https://quanteda.io" target="_blank" rel="noopener">quanteda</a> library, which contains many helpful methods for text mining. Besides statistical analysis, NLP techniques like stemming and lemmatization can be used. One of the most helpful ways to compare the most used words in tweets that have been labeled as disaster or not is to plot a wordcloud.</p><img src="/2020/Machine-learning-basics/wordcloud.png" title="Wordcloud"><p>The data was not clean, there were duplicate tweets with different labels, so the best approach is to use the mode to select the right label. To try a ML algorithm, text must be transformed to tabular data, where each row is a tweet, or document, and each column is a word, or feature. For the feature value I had best results with the word count, but binary encoding or tf-idf are also valid choices. Additional features like number of URLs and # also improved model performance.</p><p>Since the resulting matrix is sparse and has many dimensions, it is helpful to remove words/columns that rarely appear. Naive bayes and Linear SVM algorithms are often used for text classification because they are fast to train on high dimensional data and provide good results. For other algorithms to train faster, a dimensionality reduction algorithm is typically applied first. With SVM I was able to reach 83% CV accuracy and 80% test accuracy. Bigrams did not improve the model but ensembles probably could. Unsupervised techniques like <a href="https://nlp.stanford.edu/projects/glove" target="_blank" rel="noopener">GloVe</a> to create word embeddings are quite interesting.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>I am satisfied with the results obtained and was able to learn more about machine learning. I did not explain everything and did not include any code on this post, feel free to check the code on <a href="https://github.com/ruial/kaggle-problems" target="_blank" rel="noopener">github</a> which contains more comments.</p>]]></content>
      
      
      
        <tags>
            
            <tag> data </tag>
            
            <tag> R </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Query MongoDB using SQL with Presto</title>
      <link href="/2019/Query-MongoDB-using-SQL-with-Presto/"/>
      <url>/2019/Query-MongoDB-using-SQL-with-Presto/</url>
      
        <content type="html"><![CDATA[<p>With an increasing number of specialized databases, each having their own query languages, data analysts have a hard time to combine data from multiples sources. To mitigate this issue, Facebook created <a href="https://prestosql.io" target="_blank" rel="noopener">Presto</a>, a high performance, distributed SQL query engine for big data. I will be creating a small simulation using <a href="https://www.metabase.com" target="_blank" rel="noopener">Metabase</a>, a web based open-source BI solution to visualize data from <a href="https://www.mongodb.com" target="_blank" rel="noopener">MongoDB</a>.</p><h2 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h2><p><a href="https://www.docker.com" target="_blank" rel="noopener">Docker</a> is a container platform. Containers package software applications and all their dependencies, which helps enable reproducible infrastructure. <a href="https://hub.docker.com" target="_blank" rel="noopener">Docker Hub</a> contains many container images so we have a starting point and don’t have to package everything ourselves. This is extremely helpful, specially during development.</p><p>Each container should only have one function. For example, a simple scenario when doing back-end development is to use a container for the application and another for the database. To connect the containers in a local environment, Docker Compose is the easiest solution but for production, the recommended approach is to use an orchestration system like <a href="https://kubernetes.io" target="_blank" rel="noopener">Kubernetes</a>.</p><h2 id="Presto"><a href="#Presto" class="headerlink" title="Presto"></a>Presto</h2><p>Following the rise of NoSQL databases, many specialized query languages exist today. This leads to huge data integration efforts and expensive ETL processes. In the Hadoop world, a number of solutions emerged to enable the usage of SQL to retrieve data, Presto being the most interesting in my opinion.</p><p>The main advantage of Presto is that it has many data connectors, such as Kafka, Cassandra, Elasticsearch, MongoDB, Postgres, etc… It is then able to infer the schema automatically and handle semi-structured data. Arrays, nested objects, multi-database joins and the regular SQL operations are all supported.</p><p><a href="https://drill.apache.org" target="_blank" rel="noopener">Apache Drill</a> is a very similar alternative to Presto, however it appears to be less popular and doesn’t support as many data sources. Performance comparisons are out of the scope for this post, but Presto is used by big players in data-intensive environments. Amazon created <a href="https://aws.amazon.com/athena" target="_blank" rel="noopener">Athena</a> which is based on Presto and heavily integrated in AWS.</p><h2 id="Metabase"><a href="#Metabase" class="headerlink" title="Metabase"></a>Metabase</h2><p>There is a lot of commercial Business Intelligence software out there. Metabase stands out for being an open-source BI technology that is easy to use even for people that don’t know SQL, allowing them to explore the data and create web dashboards. Some advanced visualizations still require SQL knowledge.</p><p>Other alternatives include <a href="https://redash.io" target="_blank" rel="noopener">Redash</a> and <a href="https://superset.incubator.apache.org" target="_blank" rel="noopener">Superset</a>. From my research, these support more chart types but are less user friendly and deployment is a bit more complicated.</p><h2 id="MongoDB"><a href="#MongoDB" class="headerlink" title="MongoDB"></a>MongoDB</h2><p>MongoDB is a document-oriented database. Documents are stored as JSON objects, making it a good choice for semi-structured data with a flexible schema. It does not support SQL out of the box, which makes it harder for analysts to extract data because they have to learn another query language.</p><h2 id="Simulation"><a href="#Simulation" class="headerlink" title="Simulation"></a>Simulation</h2><p>I created 4 containers, 2 for different databases, 1 for Presto and the last for Metabase. Application data should be stored in volumes. Configuration files can either be copied and stored as part of the image or we can follow the volume approach. On Windows, named volumes must be used in some cases because of file permission issues.</p><figure class="highlight yaml"><figcaption><span>docker-compose.yml</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">'3.4'</span></span><br><span class="line"></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line"><span class="attr">  mongo:</span></span><br><span class="line"><span class="attr">    build:</span> <span class="string">./mongo</span></span><br><span class="line"><span class="attr">    container_name:</span> <span class="string">mongo</span></span><br><span class="line"><span class="attr">    volumes:</span></span><br><span class="line"><span class="attr">      - mongo-data:</span><span class="string">/data/db</span></span><br><span class="line"><span class="attr">  postgres:</span></span><br><span class="line"><span class="attr">    build:</span> <span class="string">./postgres</span></span><br><span class="line"><span class="attr">    container_name:</span> <span class="string">postgres</span></span><br><span class="line"><span class="attr">    volumes:</span></span><br><span class="line"><span class="attr">      - postgres-data:</span><span class="string">/var/lib/postgresql/data</span></span><br><span class="line"><span class="attr">    environment:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">POSTGRES_PASSWORD=secret123</span></span><br><span class="line"><span class="attr">  presto:</span></span><br><span class="line"><span class="attr">    build:</span> <span class="string">./presto</span></span><br><span class="line"><span class="attr">    container_name:</span> <span class="string">presto</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"8080:8080"</span></span><br><span class="line"><span class="attr">  metabase:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">metabase/metabase:v0.33.4</span></span><br><span class="line"><span class="attr">    container_name:</span> <span class="string">metabase</span></span><br><span class="line"><span class="attr">    volumes:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">./data/metabase:/metabase-data</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"8000:3000"</span></span><br><span class="line"><span class="attr">    environment:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">MB_DB_FILE=/metabase-data/metabase.db</span></span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"><span class="attr">volumes:</span></span><br><span class="line"><span class="attr">  mongo-data:</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">mongo-data</span></span><br><span class="line"><span class="attr">  postgres-data:</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">postgres-data</span></span><br></pre></td></tr></table></figure><p>I’m including here some commands that I used for testing. <a href="https://www.json-generator.com" target="_blank" rel="noopener">JSON Generator</a> is a very nice tool to create datasets. For more details about how to seed the databases and configure Presto, check the repository.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it mongo bash</span><br><span class="line"></span><br><span class="line"><span class="comment"># mongo</span></span><br><span class="line">mongo</span><br><span class="line"><span class="keyword">show</span> dbs</span><br><span class="line"><span class="keyword">use</span> testdb</span><br><span class="line"><span class="keyword">show</span> collections</span><br><span class="line">db.orders.count()</span><br><span class="line">db.users.find().limit(<span class="number">2</span>).pretty()</span><br><span class="line"></span><br><span class="line"><span class="comment"># postgres</span></span><br><span class="line">su postgres</span><br><span class="line">psql</span><br><span class="line">\l  <span class="comment"># list dbs</span></span><br><span class="line">\c locationdb <span class="comment"># connect to locationdb</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> information_schema.schemata; <span class="comment"># list all database schemas</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> pg_tables <span class="keyword">where</span> schemaname=<span class="string">'userlocation'</span>; <span class="comment"># list all tables from schema</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> userlocation.city;</span><br><span class="line"></span><br><span class="line"><span class="comment"># presto</span></span><br><span class="line">presto</span><br><span class="line"><span class="keyword">show</span> catalogs;</span><br><span class="line"><span class="keyword">show</span> schemas <span class="keyword">from</span> mongo;</span><br><span class="line"><span class="keyword">show</span> <span class="keyword">tables</span> <span class="keyword">from</span> mongo.testdb; </span><br><span class="line"><span class="keyword">describe</span> mongo.testdb.orders;</span><br><span class="line"></span><br><span class="line"><span class="comment"># select all orders, including the mongo _id</span></span><br><span class="line"><span class="keyword">select</span> _id, * <span class="keyword">from</span> mongo.testdb.orders <span class="keyword">limit</span> <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># number of orders</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> mongo.testdb.orders;</span><br><span class="line"></span><br><span class="line"><span class="comment"># number of items from a certain order</span></span><br><span class="line"><span class="keyword">select</span> cardinality(items) <span class="keyword">from</span> mongo.testdb.orders <span class="keyword">where</span> <span class="keyword">id</span> = <span class="string">'s1'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># average number of items from shooping cart</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">avg</span>(cardinality(items)) <span class="keyword">from</span> mongo.testdb.orders;</span><br><span class="line"></span><br><span class="line"><span class="comment"># total number of items by user</span></span><br><span class="line"><span class="keyword">select</span> userId, <span class="keyword">sum</span>(cardinality(items)) n <span class="keyword">from</span> mongo.testdb.orders <span class="keyword">group</span> <span class="keyword">by</span> userId <span class="keyword">order</span> <span class="keyword">by</span> -n;</span><br><span class="line"></span><br><span class="line"><span class="comment"># orders between 2 dates, for some reason have to add an OR condition for it to work correctly, maybe a bug?</span></span><br><span class="line"><span class="keyword">select</span> _id, <span class="keyword">id</span>, <span class="string">"when"</span>, <span class="keyword">year</span>(<span class="string">"when"</span>) y, <span class="keyword">month</span>(<span class="string">"when"</span>) m, <span class="keyword">day</span>(<span class="string">"when"</span>) d <span class="keyword">from</span> mongo.testdb.orders <span class="keyword">where</span> _id <span class="keyword">is</span> <span class="literal">null</span> <span class="keyword">or</span> <span class="string">"when"</span> <span class="keyword">between</span> <span class="built_in">timestamp</span> <span class="string">'2019-10-28'</span> <span class="keyword">and</span> <span class="built_in">timestamp</span> <span class="string">'2019-11-02'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># unnest objects in arrays</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, userId, <span class="string">"when"</span>, <span class="keyword">name</span>, price, quantity <span class="keyword">from</span> mongo.testdb.orders <span class="keyword">CROSS</span> <span class="keyword">JOIN</span> <span class="keyword">UNNEST</span>(items) <span class="keyword">AS</span> t (<span class="keyword">name</span>, price, quantity) <span class="keyword">where</span> <span class="keyword">name</span> <span class="keyword">like</span> <span class="string">'%sic%'</span> <span class="keyword">limit</span> <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># total money spent by users</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">sum</span>(price * quantity) total <span class="keyword">from</span> mongo.testdb.orders <span class="keyword">CROSS</span> <span class="keyword">JOIN</span> <span class="keyword">UNNEST</span>(items) <span class="keyword">AS</span> t (<span class="keyword">name</span>, price, quantity);</span><br><span class="line"></span><br><span class="line"><span class="comment"># select unique users that had orders and join info with other collection and table in other database</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">distinct</span>(userId), <span class="keyword">name</span>, age, location, country <span class="keyword">from</span> mongo.testdb.orders o <span class="keyword">left</span> <span class="keyword">join</span> mongo.testdb.users u <span class="keyword">on</span> o.userId = u.id <span class="keyword">left</span> <span class="keyword">join</span> postgres.userlocation.city l <span class="keyword">on</span> u.location = l.city_name;</span><br><span class="line"></span><br><span class="line"><span class="comment"># location info for cities that have users - subqueries demo</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> postgres.userlocation.city <span class="keyword">where</span> city_name <span class="keyword">in</span> (<span class="keyword">select</span> location <span class="keyword">from</span> mongo.testdb.users);</span><br><span class="line"></span><br><span class="line"><span class="comment"># metabase - number of orders by month</span></span><br><span class="line"><span class="keyword">SELECT</span> date_trunc(<span class="string">'month'</span>, <span class="string">"testdb"</span>.<span class="string">"orders"</span>.<span class="string">"when"</span>) <span class="keyword">AS</span> <span class="string">"when"</span>, <span class="keyword">sum</span>(price * quantity) <span class="keyword">AS</span> <span class="string">"total"</span></span><br><span class="line"><span class="keyword">FROM</span> <span class="string">"testdb"</span>.<span class="string">"orders"</span></span><br><span class="line"><span class="keyword">CROSS</span> <span class="keyword">JOIN</span> <span class="keyword">UNNEST</span>(items) <span class="keyword">AS</span> t (<span class="keyword">name</span>, price, quantity)</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> date_trunc(<span class="string">'month'</span>, <span class="string">"testdb"</span>.<span class="string">"orders"</span>.<span class="string">"when"</span>)</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> date_trunc(<span class="string">'month'</span>, <span class="string">"testdb"</span>.<span class="string">"orders"</span>.<span class="string">"when"</span>) <span class="keyword">ASC</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># copy data from named volume to host</span></span><br><span class="line">docker run -v postgres-<span class="keyword">data</span>:/volumedata <span class="comment">--name test --rm -it alpine sh</span></span><br><span class="line">docker cp <span class="keyword">test</span>:/volumedata ./mydata</span><br><span class="line"></span><br><span class="line"><span class="comment"># stop containers and remove a volume</span></span><br><span class="line">docker <span class="keyword">container</span> prune</span><br><span class="line">docker volume rm postgres-<span class="keyword">data</span></span><br></pre></td></tr></table></figure><h3 id="Metabase-setup"><a href="#Metabase-setup" class="headerlink" title="Metabase setup"></a>Metabase setup</h3><p>Unlike the other tools, on Metabase there is no way to import/export dashboard configurations, other than copying the database it uses, which is H2 by default. For that reason I have included the data folder in the project repository.</p><img src="/2019/Query-MongoDB-using-SQL-with-Presto/metabase-setup.png" title="Metabase setup"><h3 id="Presto-UI"><a href="#Presto-UI" class="headerlink" title="Presto UI"></a>Presto UI</h3><p>It is possible to see the queries that are executed by the Presto cluster. A distributed system is required to handle large amounts of data.</p><img src="/2019/Query-MongoDB-using-SQL-with-Presto/presto-ui.png" title="Presto UI"><h3 id="Dashboard"><a href="#Dashboard" class="headerlink" title="Dashboard"></a>Dashboard</h3><p>This final dashboard can be obtained by navigating to <em>localhost:8000</em> after running <em>docker-compose up</em>. During the first manual setup I chose <em>test[at]example.com</em> as the username and <em>test1234</em> for the password. Ideally this should be a configuration of the image or automatically setup when the container starts.</p><img src="/2019/Query-MongoDB-using-SQL-with-Presto/dashboard-example.png" title="Simulation dashboard"><h2 id="Closing-thoughts"><a href="#Closing-thoughts" class="headerlink" title="Closing thoughts"></a>Closing thoughts</h2><p>I recently had to create a web dashboard that needed data from multiple databases and these technologies allowed me to quickly build a quality prototype. Feel free to check the <a href="https://github.com/ruial/presto-simulation" target="_blank" rel="noopener">repository</a>.</p>]]></content>
      
      
      
        <tags>
            
            <tag> sql </tag>
            
            <tag> mongodb </tag>
            
            <tag> presto </tag>
            
            <tag> metabase </tag>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Extracting and analysing data using R</title>
      <link href="/2019/Extracting-and-analysing-data-using-R/"/>
      <url>/2019/Extracting-and-analysing-data-using-R/</url>
      
        <content type="html"><![CDATA[<p>To finish a Master’s degree, students have to write a dissertation. As I’m on that path and out of curiosity, I wanted to get some insights on the dissertations written on my university in the last years. This was also a great opportunity to practice some data extraction and visualization skills that I learned by reading the free book <a href="https://r4ds.had.co.nz" target="_blank" rel="noopener">R for Data Science</a>.</p><h2 id="Extracting-public-data"><a href="#Extracting-public-data" class="headerlink" title="Extracting public data"></a>Extracting public data</h2><p>Universities in Portugal publish student dissertations on the <a href="https://renates.dgeec.mec.pt" target="_blank" rel="noopener">Renates platform</a>. We can freely search and download these works with some limitations, as the download can be restricted (ex: contains information that a company does not want made public yet), but some data is always available. Search is limited by 200 results, so a search by year and course is necessary.</p><p>To export the data, a convenient excel spreadsheet can be downloaded. I had some problems with the files as I couldn’t use the <strong>readxl</strong> library, although it supported the legacy <em>.xls</em> format. As such, I opted to open Excel and re-save the files as there were only a few of them. There is a column with the repository url to a page that contains a link with the dissertation pdf, which had to be scraped using the <strong>rvest</strong> library. I did not parallelize the downloads to not overload the servers, so the process took a few minutes.</p><p>Here’s a glimpse of the wrangled data:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">## Observations: 292</span><br><span class="line">## Variables: 13</span><br><span class="line">## $ ID             &lt;dbl&gt; 201812975, 201813238, 201813297, 201813360, 201...</span><br><span class="line">## $ Sex            &lt;chr&gt; &quot;Feminino&quot;, &quot;Masculino&quot;, &quot;Masculino&quot;, &quot;Feminino...</span><br><span class="line">## $ Nationality    &lt;chr&gt; &quot;Portugal&quot;, &quot;Portugal&quot;, &quot;Portugal&quot;, &quot;Portugal&quot;,...</span><br><span class="line">## $ Specialization &lt;chr&gt; &quot;Engenharia de Software&quot;, &quot;Engenharia de Softwa...</span><br><span class="line">## $ Title          &lt;chr&gt; &quot;Evaluating the Combination of Relaxation and A...</span><br><span class="line">## $ Keywords       &lt;chr&gt; &quot;Negociação de mapeamentos de ontologias; Argum...</span><br><span class="line">## $ Date           &lt;dttm&gt; 2013-11-15, 2013-11-14, 2013-11-13, 2013-11-13...</span><br><span class="line">## $ Grade          &lt;dbl&gt; 16, 16, 15, 15, 14, 14, 15, 14, 15, 17, 13, 14,...</span><br><span class="line">## $ Advisors       &lt;chr&gt; &quot;Name of an advisor&quot;, &quot;Name of another advisor....</span><br><span class="line">## $ Pages          &lt;dbl&gt; 93, 139, 145, 179, 87, 136, 124, 123, 108, 173,...</span><br><span class="line">## $ Words          &lt;dbl&gt; 22292, 39826, 36143, 40198, 19187, 22224, 29549...</span><br><span class="line">## $ WordsPerPage   &lt;dbl&gt; 239.6989, 286.5180, 249.2621, 224.5698, 220.540...</span><br><span class="line">## $ Size           &lt;dbl&gt; 1.835146, 4.263048, 3.703902, 2.768955, 2.92813...</span><br></pre></td></tr></table></figure><h2 id="Exploratory-data-analysis"><a href="#Exploratory-data-analysis" class="headerlink" title="Exploratory data analysis"></a>Exploratory data analysis</h2><h3 id="Male-and-female-students-by-year"><a href="#Male-and-female-students-by-year" class="headerlink" title="Male and female students by year"></a>Male and female students by year</h3><img src="/2019/Extracting-and-analysing-data-using-R/compare-sex.png" title="Compare sex by year"><p>There are 262 male students and 30 female students that finished their disserations between 2013 and 2018. More than 100 students are enrolling each year, so the number of dissertations is a bit low.</p><h3 id="Specializations"><a href="#Specializations" class="headerlink" title="Specializations"></a>Specializations</h3><img src="/2019/Extracting-and-analysing-data-using-R/specializations.png" title="Specializations"><p>We have 6 different specializations, but 2 of them were discontinued/renamed, I know that “Arquitetura, Sistemas e Redes” is similar to “Sistemas Computacionais” and “Tecnologias do Conhecimento e Decisão” is the same as “Sistemas de Informação e Conhecimento”, so I will make those transformations for this analysis.</p><h3 id="Grade-distribution-by-specialization"><a href="#Grade-distribution-by-specialization" class="headerlink" title="Grade distribution by specialization"></a>Grade distribution by specialization</h3><img src="/2019/Extracting-and-analysing-data-using-R/specializations-boxplots.png" title="Compare grades by specialization"><p>No significant difference in grades between different specializations.</p><h3 id="Average-number-of-pages-by-specialization"><a href="#Average-number-of-pages-by-specialization" class="headerlink" title="Average number of pages by specialization"></a>Average number of pages by specialization</h3><img src="/2019/Extracting-and-analysing-data-using-R/pages-histogram.png" title="Number of pages distribution"><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">summary(data$Pages)</span><br><span class="line"><span class="comment">##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's</span></span><br><span class="line"><span class="comment">##   59.00   98.75  119.00  122.67  144.00  330.00      44</span></span><br></pre></td></tr></table></figure><p>On average people write about 123 pages.</p><h3 id="Advisors-with-most-students-and-grade-distributions-with-specializations"><a href="#Advisors-with-most-students-and-grade-distributions-with-specializations" class="headerlink" title="Advisors with most students and grade distributions with specializations"></a>Advisors with most students and grade distributions with specializations</h3><img src="/2019/Extracting-and-analysing-data-using-R/advisors.png" title="Advisors"><p>The advisors assist students from multiple specializations and grades are well distributed. A bit of jittering and transparency was used in this categorical scatter plot to aid in interpretation.</p><h3 id="Correlation-matrix"><a href="#Correlation-matrix" class="headerlink" title="Correlation matrix"></a>Correlation matrix</h3><img src="/2019/Extracting-and-analysing-data-using-R/correlation-matrix.png" title="Correlation matrix"><p>The number of words correlates with the number of pages as expected.</p><h3 id="Compare-grades-and-number-of-words"><a href="#Compare-grades-and-number-of-words" class="headerlink" title="Compare grades and number of words"></a>Compare grades and number of words</h3><img src="/2019/Extracting-and-analysing-data-using-R/grades-words.png" title="Compare grades and number of words"><p>There is some difference in the distributions, but I would like more data as most works are graded between 14 and 16. Update: these grades are actually final course grades, not the dissertation grades.</p><h3 id="Common-keywords"><a href="#Common-keywords" class="headerlink" title="Common keywords"></a>Common keywords</h3><img src="/2019/Extracting-and-analysing-data-using-R/wordcloud.png" title="Keywords"><p>Artificial intelligence, mobile systems and web development seem to be hot topics.</p><h2 id="Closing-thoughts"><a href="#Closing-thoughts" class="headerlink" title="Closing thoughts"></a>Closing thoughts</h2><p><strong>R</strong> and <strong>tidyverse</strong> are very helpful tools for data visualization and exploration. I have recently gained an interest on distributed systems and machine learning, so for the latter I will be signing up on <a href="https://www.kaggle.com" target="_blank" rel="noopener">Kaggle</a> and learn some new things on my free time. The full analysis, which is mainly focused on creating different visualizations, is available on <a href="https://github.com/ruial/dissertations-eda" target="_blank" rel="noopener">Github</a>.</p>]]></content>
      
      
      
        <tags>
            
            <tag> data </tag>
            
            <tag> R </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Game hacking on Linux</title>
      <link href="/2019/Game-hacking-on-Linux/"/>
      <url>/2019/Game-hacking-on-Linux/</url>
      
        <content type="html"><![CDATA[<p>I always liked to play computer games. They are complex pieces of software. In the past most games had cheat codes, but nowadays it’s less common. As an engineer I like to see how things work, so let’s reverse engineer an open-source game on Linux called <a href="https://www.xonotic.org" target="_blank" rel="noopener">Xonotic</a> and create a small cheat to have infinite health and ammo.</p><h2 id="How-are-game-hacks-made"><a href="#How-are-game-hacks-made" class="headerlink" title="How are game hacks made"></a>How are game hacks made</h2><p>To completely understand how cheats are made, some knowledge about how programs and memory work is valuable. Most common operating systems allow processes to read and write memory on other processes, which can be used to cheat in games. Values such as health are often stored in dynamically allocated memory. This means that when the game is restarted, the memory address that keeps the health will change. However, there is always some static base address that points to the health address, we just have to follow the pointers using static offsets.</p><h2 id="Cheat-Engine"><a href="#Cheat-Engine" class="headerlink" title="Cheat Engine"></a>Cheat Engine</h2><p>The most popular tool to hack games is <a href="https://www.cheatengine.org" target="_blank" rel="noopener">Cheat Engine</a>. It is an open-source memory scanner and debugger. As most games on PC are for Windows, that is the primary focus of the software. On Linux it uses a client-server architecture so we must download the Linux server and also the Windows client, which must be executed on <a href="https://www.winehq.org" target="_blank" rel="noopener">Wine</a>.</p><h3 id="Searching-the-health"><a href="#Searching-the-health" class="headerlink" title="Searching the health"></a>Searching the health</h3><p>The first step is to start the cheat engine server using sudo and then the client. Afterwards connect to the server on <em>File &gt; Open Process &gt; Network &gt; Connect</em> and select the game process.</p><p>Now we can search for the health. Start with 100, scan, take a bit of damage, scan again until we have few addresses. Green addresses are static and finding them so soon usually means that it is not the address we want. Let’s try the other address and “Find out what writes to this address”.</p><img src="/2019/Game-hacking-on-Linux/health-search.png" title="Health search"><p>A bit of assembly knowledge is useful. We can see that the <strong>mov</strong> instruction copies the value from the <strong>ecx</strong> register to the address we found. On the line above, the <strong>ecx</strong> value is copied from the <strong>rdi</strong> register, which contains the address <strong>0x7FFCE0634490</strong>. As <strong>rax</strong> is 0, the addition and multiplication do nothing.</p><img src="/2019/Game-hacking-on-Linux/writes-to-health.png" title="Writes to health"><p>Let’s add the address we found and “Find out what accesses to this address” and we can see that many instructions access this address. Since the health is being copied from here, we will have to search all of them until we find some register with the value 100 (decimal), which will be 0x64 (hexadecimal). Luckily, I didn’t have to search a lot and then I clicked the “Show disassembler” to open the Memory viewer. In here we can place some breakpoints and debug the program.</p><p>This is very insightful! The <strong>cvttss2si</strong> instruction is used to convert floats to integers. Going back a little we find the <strong>lea</strong> instruction which is copying the health from the address in <strong>edx + rax * 4</strong>. With the multiplication we get our first offset, <strong>0x30 * 0x4 = 0xC0</strong>. The <strong>rdx</strong> is set based on <strong>rd12 + 08</strong>, which is <strong>0xBE4E988</strong>. We can verify this by adding the pointer and offset to the list of addresses and by setting the type to float.</p><img src="/2019/Game-hacking-on-Linux/memory-viewer.png" title="Memory viewer"><p>Now we need to find out what accesses to this pointer. These steps of finding the base address may involve trial and error. Let’s pick the first instruction and we know from the <strong>mov</strong> instruction that the offset is <strong>rsi + 08</strong>, which equals <strong>0x18</strong>. The <strong>r14</strong> register has an address and by looking at an instruction above, it is obtained through <strong>rbx + 0x5C360</strong> and <strong>rbx</strong> is <strong>0x1E6AAA0</strong>. As this address is static, it is our base address to get to the health, by applying the correct offsets.</p><img src="/2019/Game-hacking-on-Linux/memory-viewer-pointer.png" title="Memory viewer pointer"><h3 id="Pointer-scan"><a href="#Pointer-scan" class="headerlink" title="Pointer scan"></a>Pointer scan</h3><p>An alternative to this backtracking is when we find the real health address, we do a pointer scan. We see 2 different pointer paths, to pick the right one we can restart the game and see which still points to the health. The static base address with the offset <strong>0x18</strong> is the same as the one previously found but is getting calculated using the “xonotic-linux64-sdl” module address.</p><img src="/2019/Game-hacking-on-Linux/pointer-scan.png" title="Pointer scan"><h3 id="Dissect-data-structures"><a href="#Dissect-data-structures" class="headerlink" title="Dissect data structures"></a>Dissect data structures</h3><p>To find the ammo, I took a quick shortcut. Usually games store the player data in a <strong>struct</strong> or a <strong>class</strong> and as such, it’s highly likely that the health and ammo are in close memory proximity. By using the <strong>Dissect data structures</strong> feature from the <em>Memory Viewer &gt; Tools &gt; Dissect data/structures &gt; Structures &gt; Define new structure</em> we find that the ammo is just a few bytes away from the health, with the value 15.</p><img src="/2019/Game-hacking-on-Linux/dissect.png" title="Dissect data structures"><h2 id="Coding"><a href="#Coding" class="headerlink" title="Coding"></a>Coding</h2><p>To read and write memory from other processes we need to call APIs that depend on the operating system. For Linux we can use <a href="http://man7.org/linux/man-pages/man2/ptrace.2.html" target="_blank" rel="noopener">ptrace</a> or <a href="https://linux.die.net/man/2/process_vm_readv" target="_blank" rel="noopener">process_vm_readv</a> and <a href="https://linux.die.net/man/2/process_vm_writev" target="_blank" rel="noopener">process_vm_writev</a>. On Windows, the functions <a href="https://docs.microsoft.com/en-us/windows/win32/api/memoryapi/nf-memoryapi-readprocessmemory" target="_blank" rel="noopener">ReadProcessMemory</a> and <a href="https://docs.microsoft.com/en-us/windows/win32/api/memoryapi/nf-memoryapi-writeprocessmemory" target="_blank" rel="noopener">WriteProcessMemory</a> are available. Alternatively, a module (.so/.dll) can be injected into the game to avoid using these APIs and enable direct memory access. The best language for these low level things is C or C++.</p><figure class="highlight c"><figcaption><span>Read and write memory helpers</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">read_addr</span><span class="params">(<span class="keyword">pid_t</span> pid, <span class="keyword">unsigned</span> <span class="keyword">long</span> addr, <span class="keyword">void</span> *buffer, <span class="keyword">size_t</span> size)</span> </span>&#123;</span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">iovec</span> <span class="title">local</span>[1];</span></span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">iovec</span> <span class="title">remote</span>[1];</span></span><br><span class="line"></span><br><span class="line">  local[<span class="number">0</span>].iov_base = buffer;</span><br><span class="line">  local[<span class="number">0</span>].iov_len = size;</span><br><span class="line">  remote[<span class="number">0</span>].iov_base = (<span class="keyword">void</span> *)addr;</span><br><span class="line">  remote[<span class="number">0</span>].iov_len = size;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> process_vm_readv(pid, local, <span class="number">1</span>, remote, <span class="number">1</span>, <span class="number">0</span>) == size;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">write_addr</span><span class="params">(<span class="keyword">pid_t</span> pid, <span class="keyword">unsigned</span> <span class="keyword">long</span> addr, <span class="keyword">void</span> *buffer, <span class="keyword">size_t</span> size)</span> </span>&#123;</span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">iovec</span> <span class="title">local</span>[1];</span></span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">iovec</span> <span class="title">remote</span>[1];</span></span><br><span class="line"></span><br><span class="line">  local[<span class="number">0</span>].iov_base = buffer;</span><br><span class="line">  local[<span class="number">0</span>].iov_len = size;</span><br><span class="line">  remote[<span class="number">0</span>].iov_base = (<span class="keyword">void</span> *)addr;</span><br><span class="line">  remote[<span class="number">0</span>].iov_len = size;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> process_vm_writev(pid, local, <span class="number">1</span>, remote, <span class="number">1</span>, <span class="number">0</span>) == size;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Using system calls is an expensive operation. As such, it is better to create a <strong>struct</strong> to hold the player information and read one bigger chunk of memory at once, than many small chunks. As we are getting to the dynamic player structure address by reading pointers through a static base address and offsets, the cheat will always work when the game is restarted, however these offsets may change when the game is updated. There are alternatives to get to the dynamic address that may resist game updates which are based on signature/AOB(array of bytes) scans.</p><figure class="highlight c"><figcaption><span>Memory structures, read struct and write health</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PLAYER_OFFSET_1 0x1AC6E00</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PLAYER_OFFSET_2 0x18</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> HEALTH_OFFSET 0xC0</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> AMMO_OFFSET 0xD8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">  <span class="keyword">bool</span> health;</span><br><span class="line">  <span class="keyword">bool</span> ammo;</span><br><span class="line">&#125; Options;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">  <span class="keyword">pid_t</span> pid;</span><br><span class="line">  <span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">module</span>;</span><br><span class="line">  Options options;</span><br><span class="line">&#125; Game;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">  <span class="keyword">char</span> _1[<span class="number">0xC0</span>];</span><br><span class="line">  <span class="keyword">float</span> health;  <span class="comment">// 0xC0</span></span><br><span class="line">  <span class="keyword">char</span> _2[<span class="number">0x14</span>]; <span class="comment">// 0xD8 - 0xC0 - 0x4(sizeof float)</span></span><br><span class="line">  <span class="keyword">float</span> ammo;    <span class="comment">// 0xD8</span></span><br><span class="line">  <span class="keyword">unsigned</span> <span class="keyword">long</span> address;</span><br><span class="line">&#125; MyPlayer;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">readMyPlayer</span><span class="params">(Game game, MyPlayer *myPlayer)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">unsigned</span> <span class="keyword">long</span> ptr;</span><br><span class="line">  <span class="keyword">if</span> (read_addr(game.pid, game.<span class="keyword">module</span> + PLAYER_OFFSET_1, &amp;ptr, <span class="keyword">sizeof</span>(ptr))) &#123;</span><br><span class="line">    <span class="keyword">if</span> (read_addr(game.pid, ptr + PLAYER_OFFSET_2, &amp;ptr, <span class="keyword">sizeof</span>(ptr))) &#123;</span><br><span class="line">      myPlayer-&gt;address = ptr;</span><br><span class="line">      <span class="keyword">return</span> read_addr(game.pid, ptr, myPlayer,</span><br><span class="line">                       <span class="keyword">sizeof</span>(MyPlayer) - <span class="keyword">sizeof</span>(myPlayer-&gt;address));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">writeHealth</span><span class="params">(Game game, MyPlayer myPlayer, <span class="keyword">float</span> value)</span> </span>&#123;</span><br><span class="line">  write_addr(game.pid, myPlayer.address + HEALTH_OFFSET, &amp;value, <span class="keyword">sizeof</span>(value));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Cheats are usually running in some infinite loop until they detect the game is not available. Sleep commands avoid hitting unnecessary 100% CPU usage. Global keyboard input detection to toggle features is helpful (code is OS dependent). An alternative to writing the health constantly, would be to patch the code that decreases it, like replacing a <strong>mov</strong> with <strong>nop</strong> instructions, which could be done by writing the correct bytes at the correct address.</p><figure class="highlight c"><figcaption><span>Main loop</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">open_game</span><span class="params">(Game *game)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">while</span> ((game-&gt;pid = find_pid(<span class="string">"xonotic-linux64-sdl"</span>)) == <span class="number">0</span>) &#123;</span><br><span class="line">    sleep_ms(<span class="number">1000</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">while</span> ((game-&gt;<span class="keyword">module</span> = module_addr(game-&gt;pid, <span class="string">"xonotic-linux64-sdl"</span>)) == <span class="number">0</span>) &#123;</span><br><span class="line">    sleep_ms(<span class="number">1000</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"waiting for xonotic...\n"</span>);</span><br><span class="line">  Game game;</span><br><span class="line">  open_game(&amp;game);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"pid: %d\n"</span>, game.pid);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"xonotic-linux64-sdl module: %lx\n"</span>, game.<span class="keyword">module</span>);</span><br><span class="line"></span><br><span class="line">  MyPlayer myPlayer;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (readMyPlayer(game, &amp;myPlayer)) &#123;</span><br><span class="line">      manage_input(&amp;game);</span><br><span class="line">      <span class="keyword">if</span> (game.options.health) &#123;</span><br><span class="line">        writeHealth(game, myPlayer, <span class="number">150</span>);</span><br><span class="line">      &#125;</span><br><span class="line">      printPlayer(myPlayer);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">"not in arena\n"</span>);</span><br><span class="line">      sleep_ms(<span class="number">1000</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    sleep_ms(<span class="number">50</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Keep in mind that this hack works for single player only. Server side software should keep their own health value for each player and as such we can’t change it, any local change will be visual only.</p><h2 id="Closing-thoughts"><a href="#Closing-thoughts" class="headerlink" title="Closing thoughts"></a>Closing thoughts</h2><p>Reverse engineering is hard. I admire the researchers who have to analyze software/malware in similar ways. Making complex cheats is also extremely time consuming. For example, we can draw enemies through walls or even automatically aim and shoot against them by reading their coordinates and applying some game/engine dependent math, but a lot of study is required.</p><p>The full source code is available on <a href="https://github.com/ruial/linux-game-hack-example" target="_blank" rel="noopener">GitHub</a>.</p>]]></content>
      
      
      
        <tags>
            
            <tag> hacking </tag>
            
            <tag> linux </tag>
            
            <tag> c </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Debugging mobile apps with an HTTPS proxy</title>
      <link href="/2019/Debugging-mobile-apps-with-an-https-proxy/"/>
      <url>/2019/Debugging-mobile-apps-with-an-https-proxy/</url>
      
        <content type="html"><![CDATA[<p>As a part of my job, I occasionally have to intercept HTTPS requests made from a mobile application. This is required to troubleshoot some issues and guarantee that the correct data is being sent. I will explain how I do it and talk about analytics while I briefly analyze a popular app.</p><h2 id="Analytics"><a href="#Analytics" class="headerlink" title="Analytics"></a>Analytics</h2><p>How much time users spend on the app? How many times do they return per month? Does performance have a significant impact on other metrics? Which button is clicked the most from our A/B test? These are questions that can be answered by analytics and help in making data-driven decisions which previously relied on intuition.</p><p>As storage and computing resources are getting cheaper, data collection and processing increases. Many apps track all the relevant events and aggregate this data for further analysis.</p><p>Let’s take a look at the Airbnb app. They are hitting 2 different tracking endpoints. Events are tracked regarding navigation, performance, searches, app lifecycle, and contain information about the device such as the model, screen size, network type, language, etc… On the app, these events are sent in batches to reduce bandwidth usage, which is not the case on their website as scrolling through the main page will trigger dozens of tracking requests, as we can check in the dev tools. An example of the tracked events of the app can be found in the image bellow.</p><img src="/2019/Debugging-mobile-apps-with-an-https-proxy/airbnb-tracking.png" title="Airbnb Tracking"><p>We can observe an object with a key called “advertising-ID” with the value containing several zeros. One feature not known by many people and enabled by default in Android and iOS is the advertising ID. It is a random unique identifier that ad companies can use to track users, similar to a tracking cookie on the web. In the phone settings we can request a new advertising ID or even disable it completely, forcing apps to rely on other data to track users (IP address, device fingerprinting, generated identifiers).</p><p>To disable the advertising ID follow these steps:</p><ul><li>Android<ul><li><em>Settings &gt; Google &gt; Ads &gt; Opt out of Ads Personalization</em></li></ul></li><li>iOS<ul><li><em>Settings &gt; Privacy &gt; Advertising &gt; Limit Ad tracking</em></li></ul></li></ul><h2 id="mitmproxy"><a href="#mitmproxy" class="headerlink" title="mitmproxy"></a>mitmproxy</h2><p>One useful open-source HTTPS proxy is <a href="https://mitmproxy.org" target="_blank" rel="noopener">mitmproxy</a>. It allows to intercept and modify requests through the terminal or web interface. Another popular free alternative with a focus on web security is <a href="https://portswigger.net/burp" target="_blank" rel="noopener">Burp Suite</a>. This way we can reverse engineer the APIs used by apps.</p><p>The quickest way to start mitmproxy is with Docker:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -it -v ~/.mitmproxy:/home/mitmproxy/.mitmproxy -p 8080:8080 -p 127.0.0.1:8081:8081 mitmproxy/mitmproxy mitmweb --web-iface 0.0.0.0</span><br></pre></td></tr></table></figure><p>Now we can access the UI at <em>localhost:8081</em>. To set the proxy on both iOS and Android devices, go to “<em>Settings &gt; Wi-Fi</em>“, tap the connected network and enter the computer’s IP address as the server/hostname and <em>8080</em> as the port. In case the connection is failing, the computer’s firewall may be the issue.</p><p>If the root certificate was not previously installed, visit <em>mitm.it</em> on your browser and click the image of your operating system to install the certificates. For Android, a dialog will appear to choose a certificate name and select “VPN and apps” as the “Credential use”. For iOS go to “<em>Settings &gt; General &gt; Profiles</em>“, tap on the created profile to install the certificate and then enable it in “<em>Settings &gt; General &gt; About &gt; Certificate Trust Settings</em>“.</p><p>Nowadays most popular apps employ a security technique called <a href="https://www.owasp.org/index.php/Certificate_and_Public_Key_Pinning" target="_blank" rel="noopener">Certificate Pinning</a>. If the certificate is not trusted by the app, connections are dropped. To perform man in the middle attacks we now have to tamper with the app to disable certificate pinning using toolkits like <a href="https://github.com/sensepost/objection" target="_blank" rel="noopener">objection</a> and <a href="https://www.frida.re" target="_blank" rel="noopener">Frida</a>.</p><h2 id="Closing-thoughts"><a href="#Closing-thoughts" class="headerlink" title="Closing thoughts"></a>Closing thoughts</h2><p>mitmproxy is a very useful tool. Addons like this one can be coded in Python:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mitmproxy <span class="keyword">import</span> http, ctx</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> randint</span><br><span class="line"></span><br><span class="line"><span class="comment"># simulate failure in 25% of tracking requests</span></span><br><span class="line"><span class="comment"># Usage (has live reload): mitmweb -s intercept.py</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">request</span><span class="params">(flow: http.HTTPFlow)</span>:</span></span><br><span class="line">  url = flow.request.url</span><br><span class="line">  <span class="keyword">if</span> <span class="string">'tracking'</span> <span class="keyword">in</span> url <span class="keyword">and</span> randint(<span class="number">1</span>, <span class="number">100</span>) &lt; <span class="number">25</span>:</span><br><span class="line">    ctx.log.warn(<span class="string">'Fail '</span> + url)</span><br><span class="line">    flow.response = http.HTTPResponse.make(<span class="number">500</span>, <span class="string">''</span>, &#123;&#125;)</span><br></pre></td></tr></table></figure><p>I think that analytics is very important to figure out how users are interacting with the software. The data collected can be analyzed to provide a better service by identifying issues and trends, however care should be taken to respect user’s privacy and comply with <em>GDPR</em>.</p>]]></content>
      
      
      
        <tags>
            
            <tag> analytics </tag>
            
            <tag> android </tag>
            
            <tag> ios </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/Hello-world/"/>
      <url>/2019/Hello-world/</url>
      
        <content type="html"><![CDATA[<p>A blog can be a good personal knowledge management system. Countless bookmarks, notes and files that I stored over the years are not easily searchable and shareable.</p><p>I will be using this blog to document my projects, research and share some notes that may also be helpful for other developers. Writing and publishing what I learn will also help me to improve my communication and writing skills.</p><p>On this post I will cover how to setup a static website for free.</p><h2 id="Static-site-generators"><a href="#Static-site-generators" class="headerlink" title="Static site generators"></a>Static site generators</h2><p>An easy way to create a blog is to use a CMS, such as Wordpress but static site generators are preferable for this kind of content. Since the pages are not retrieved dynamically from a database, it provides better performance, higher security and cheaper/easier scaling. Some popular static generators can be found <a href="https://www.staticgen.com" target="_blank" rel="noopener">here</a>.</p><p>Most static site generators are quite similar and even provide quick ways to migrate between them. I was inclined towards <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> and <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a> because they are fast and full-featured but ultimately went for Hexo as I am more familiar with the Node.js ecosystem and just wanted to get up and running asap.</p><h3 id="Hexo"><a href="#Hexo" class="headerlink" title="Hexo"></a>Hexo</h3><p>To create this blog and run a development server I executed the following commands</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-cli -g</span><br><span class="line">hexo init briefbytes</span><br><span class="line"><span class="built_in">cd</span> briefbytes</span><br><span class="line">npm install</span><br><span class="line">hexo server</span><br></pre></td></tr></table></figure><p>Posts are written in markdown. Additional <a href="https://hexo.io/themes/index.html" target="_blank" rel="noopener">themes</a> can be installed by cloning a theme from a repository to the <strong>themes</strong> folder:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/probberechts/hexo-theme-cactus.git themes/cactus</span><br><span class="line">rm -rf themes/cactus/.git</span><br></pre></td></tr></table></figure><p>A recommended alternative is to fork the theme and add it as a git submodule to the project, so that modifications to the theme can be made while allowing easy synchronization with the original repository.</p><p>After installing themes, they can be switched by editing the <strong>theme</strong> option in the file <strong>_config.yml</strong>. The <a href="https://hexo.io/docs/" target="_blank" rel="noopener">Hexo docs</a> are very good and provide additional information.</p><p>Some useful commands:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add post</span></span><br><span class="line">hexo new <span class="string">"post name"</span></span><br><span class="line"><span class="comment"># Add page</span></span><br><span class="line">hexo new page about</span><br><span class="line"><span class="comment"># Generate static files</span></span><br><span class="line">hexo generate</span><br></pre></td></tr></table></figure><p>Static web pages can be hosted for free in many places, within certain limits. I chose <a href="https://github.com" target="_blank" rel="noopener">GitHub</a> for version control and <a href="https://www.netlify.com" target="_blank" rel="noopener">Netlify</a> for hosting. The whole process is very simple, create a github repository, push the local changes and setup Netlify through the web interface to handle continuous integration and deployment.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Static site generators are gaining popularity because of the advantages they provide, specially for developers that are used to a Git workflow. An example of a big website using Hexo is the <a href="https://github.com/vuejs/vuejs.org" target="_blank" rel="noopener">Vue.js docs</a>. For a modern web development architecture based on client-side JavaScript, reusable APIs and prebuilt Markup, read about the <a href="https://jamstack.org" target="_blank" rel="noopener">JAMstack</a>.</p>]]></content>
      
      
      
        <tags>
            
            <tag> node.js </tag>
            
            <tag> jamstack </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
