<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>BRIEF BYTES</title>
  
  
  <link href="https://briefbytes.com/atom.xml" rel="self"/>
  
  <link href="https://briefbytes.com/"/>
  <updated>2022-11-16T20:24:29.021Z</updated>
  <id>https://briefbytes.com/</id>
  
  <author>
    <name>Rui Almeida</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Learning to rank and ML pipelines</title>
    <link href="https://briefbytes.com/2022/Learning-to-rank-and-ML-pipelines/"/>
    <id>https://briefbytes.com/2022/Learning-to-rank-and-ML-pipelines/</id>
    <published>2022-05-11T21:45:00.000Z</published>
    <updated>2022-11-16T20:24:29.021Z</updated>
    
    <content type="html"><![CDATA[<p>Ranking is a core task in information retrieval (IR) and recommender systems. In previous posts, I talked about <a href="/2021/Building-a-full-text-search-engine/" title="search engines">search engines</a> and <a href="/2020/Machine-learning-basics/" title="machine learning (ML)">machine learning (ML)</a>, and now we will see how to apply ML to rank (or re-rank) search results. Additionally, I will discuss why ML workflows should be automated and look into a few alternatives to build ML pipelines.</p><h2 id="Search-relevance"><a href="#Search-relevance" class="headerlink" title="Search relevance"></a>Search relevance</h2><p>Even when using existing technologies like Elasticsearch, engineers still have to tune their index&#x2F;queries to optimize results for relevancy. For example, when searching news, the title is more relevant than the body and recent news are more relevant than older ones. <a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/relevance-conclusion.html">Tuning</a> a search engine for optimal results can be quite challenging and requires proper instrumentation and and validation. You can find a presentation about using Logistic regression to select boost parameters <a href="https://haystackconf.com/us2021/talk-6/">here</a>.</p><p>Although machine learning is computationally expensive, it can be applied for ranking problems and return highly relevant results using plugins like <a href="https://opensourceconnections.com/blog/2017/04/03/test-drive-elasticsearch-learn-to-rank-linear-model/">Elasticsearch Learning to Rank (LTR)</a>, which supports linear models and xgboost, a tree-based model. Numerical features are required to train ML models and significantly impact performance metrics. The <a href="https://www.microsoft.com/en-us/research/project/mslr/">MSLR dataset</a> is often used to benchmark LTR models and contains query&#x2F;relevance judgments, with features like tf-idf statistics, pagerank, bm25 and so on.</p><h2 id="LTR-workflow"><a href="#LTR-workflow" class="headerlink" title="LTR workflow"></a>LTR workflow</h2><p>As you can’t improve what you can’t measure, let’s assume you already have some infrastructure in place, like an analytics solution to measure search relevancy (manual&#x2F;crowdsourced relevance judgments or click&#x2F;session data). Afterwards, you can gather the data, build features, train different models&#x2F;hyperparameters and deploy the best one. CI&#x2F;CD for ML projects is harder to get right as additional artifacts should be tracked and the models need retraining as performance drifts over time.</p><p>Python, Scala and R are among the most popular languages for ML and they’re all suitable for NLP&#x2F;Ranking as they have a healthy ecosystem of numerical packages with efficient sparse matrix representations, text processing techniques and ML algorithms. Python has the edge, especially with <a href="https://huggingface.co/docs/transformers/index">transformers</a>, while Scala is popular for data streaming (Flink, Kafka streams, Spark streaming) and R is the best for data visualization while more limited for production workloads. In <a href="https://github.com/ruial/ltr">this repo</a> you will find a Python notebook to train different LTR models on the first fold of MSLR-WEB10K, using pontwise&#x2F;pairwise&#x2F;listwise approaches, along with a comparison of Python and Scala for prediction.</p><p>These are the metrics I got on the test set with ~240k samples:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Linear model with standard scaler:</span><br><span class="line">  - ndcg@10:        0.4</span><br><span class="line">  - training:       7s</span><br><span class="line">  - python predict: 80ms</span><br><span class="line">  - scala predict:  50ms</span><br><span class="line"></span><br><span class="line">XGBoost model:</span><br><span class="line">  - ndcg@10:        0.5</span><br><span class="line">  - cpu training:   52s</span><br><span class="line">  - gpu training:   16s</span><br><span class="line">  - python predict: 800ms</span><br><span class="line">  - scala predict:  400ms</span><br></pre></td></tr></table></figure><p>The batch inference times in Python are very fast, as most code executed is actually native. I haven’t made tests in a real-time inference scenario (e.g.: REST API or queue&#x2F;stream), but even if Python is slightly slower serializing&#x2F;deserializing or transforming data, it is trivial to increase the number of instances and setup load balancing. The development effort of deploying workloads on more performant languages, such as Scala, does not seem worth it for the small latency gains. LTR is also usually applied as only a re-ranking strategy on the top K results to reduce loading times.</p><p>For more info on LTR:</p><ul><li><a href="https://tech.olx.com/ranking-ads-with-machine-learning-ee03d7734bf4">https://tech.olx.com/ranking-ads-with-machine-learning-ee03d7734bf4</a></li><li><a href="https://everdark.github.io/k9/notebooks/ml/learning_to_rank/learning_to_rank.html">https://everdark.github.io/k9/notebooks/ml/learning_to_rank/learning_to_rank.html</a></li><li><a href="https://www.manning.com/books/ai-powered-search">https://www.manning.com/books/ai-powered-search</a></li></ul><h2 id="ML-Pipelines"><a href="#ML-Pipelines" class="headerlink" title="ML Pipelines"></a>ML Pipelines</h2><p>CI&#x2F;CD systems like Jenkins or Github Actions allow workflows to be triggered via API&#x2F;Cron, but they are not suitable for ML workflows, since they lack some features&#x2F;integrations and agents may not have the required resources. A possible alternative is <a href="https://docs.aws.amazon.com/mwaa/latest/userguide/what-is-mwaa.html">Airflow</a>, which is commonly used for scheduled ETL workflows and has operators to run tasks in Kubernetes (k8s) and cloud services like AWS Batch&#x2F;ECS. I didn’t find the development experience to be the fastest (build docker image, upload image, update dag, trigger dag) and would require maintaining more glue code for ML tasks. <a href="https://orion-docs.prefect.io/">Prefect Orion</a> is a mature successor to Airflow, with a simpler architecture and better development experience, but since it is a generic workflow orchestration platform, it requires additional work to integrate with the ML ecosystem.</p><p>A viable alternative, which has a low maintenance cost, though higher lock-in, are the could offerings like AWS Sagemaker and Databricks with MLflow. However, my favorite alternative so far is <a href="https://metaflow.org/">Metaflow</a>, a modular framework to build ML pipelines. Tasks can be run locally, in AWS Batch or in k8s, allowing data scientists to develop and test the pipelines from their machine, handles dependency management with Conda, has caching, integrates with existing schedulers (AWS Step Functions, Argo Workflows), provides a web UI, and the Python client can be used to retrieve models or experiment metrics. Metaflow does not handle model deployment or monitoring, but it can be done with a specialized platform like <a href="https://github.com/SeldonIO/seldon-core">Seldon</a> or by reusing the current organization’s infrastructure. I recommend reading the excellent book <a href="https://www.manning.com/books/effective-data-science-infrastructure">Data Science Infrastructure</a> if you are interested in learning more about the topic.</p><p>These articles cover ML pipelines in more detail:</p><ul><li><a href="https://huyenchip.com/2021/09/13/data-science-infrastructure.html">https://huyenchip.com/2021/09/13/data-science-infrastructure.html</a></li><li><a href="https://ljvmiranda921.github.io/notebook/2021/05/15/navigating-the-mlops-landscape-part-2/">https://ljvmiranda921.github.io/notebook/2021/05/15/navigating-the-mlops-landscape-part-2/</a></li><li><a href="https://datatalks.club/blog/mlops-10-minutes.html">https://datatalks.club/blog/mlops-10-minutes.html</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Ranking is a core task in information retrieval (IR) and recommender systems. In previous posts, I talked about &lt;a href=&quot;/2021/Building-a</summary>
      
    
    
    
    
    <category term="data" scheme="https://briefbytes.com/tags/data/"/>
    
    <category term="python" scheme="https://briefbytes.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>The architecture of a small side project</title>
    <link href="https://briefbytes.com/2022/The-architecture-of-a-small-side-project/"/>
    <id>https://briefbytes.com/2022/The-architecture-of-a-small-side-project/</id>
    <published>2022-01-05T21:05:00.000Z</published>
    <updated>2022-11-16T20:24:29.025Z</updated>
    
    <content type="html"><![CDATA[<p>There is a huge amount of people that like to develop side projects. I mostly do it because I want to learn new things, but gaining some passive income is also an attractive idea. In the last months I launched <a href="https://www.producthunt.com/posts/ad-pruner">Ad Pruner</a>, an OpenVPN and Pi-hole as a service, and I will describe different architecture and infrastructure alternatives.</p><h2 id="Domain-registration-and-DNS"><a href="#Domain-registration-and-DNS" class="headerlink" title="Domain registration and DNS"></a>Domain registration and DNS</h2><p>This is the only part of my setup that I had to pay. To have an online business, be able to receive&#x2F;send emails and maintain complete ownership of our content, a domain name is required, preferably .com. It’s one of the reasons why I publish this blog on my own domain instead of Medium. Regarding registration, my favorite providers are Namesilo and Namecheap. I prefer to keep registration decoupled of DNS and hosting providers (there are the occasional horror stories of accounts closed due to bad fraud detection and bad support). However, Cloudflare is the cheapest registrar and they also offer free CDN, DNS and DDoS protection, so I always tend to use their free tier and never had any issues.</p><h2 id="Hosting"><a href="#Hosting" class="headerlink" title="Hosting"></a>Hosting</h2><p>For small projects it doesn’t make sense to spend a lot on hosting infrastructure. As engineers we should try to balance costs and quality. Most of the time, a cheap VM with 512MB of RAM and a throttled vCPU will work fine for a couple of low traffic websites, a small database, cache and batch jobs.</p><p>There are plenty of cheap VM&#x2F;VPS under the 5$ range, like Amazon Lightsail, Vultr or Hetzner. However, we can go even cheaper with Oracle’s free cloud offer which includes 2 AMD VMs with 1GB RAM, up to 4 ARM VMs, 10GB object storage, a load balancer, multiple IPv4 addresses, among other things. It does sound too good to be true, but I’ve been using them for months without issues. GCP always free tier also includes 1 small VM in the US and other useful things.</p><p>To receive emails on your domain, the best free solution is Zoho Mail. ImprovMX is an alternative if you are only interested in forwarding. For sending bulk email I would use Sendgrid free tier.</p><h2 id="Programming-language-and-frameworks"><a href="#Programming-language-and-frameworks" class="headerlink" title="Programming language and frameworks"></a>Programming language and frameworks</h2><p>This is mostly personal opinion, people either tend to use what they are most familiar with or what they want to learn. For rapid development, Python and Django beat most alternatives out there. Developing a separate API and a JavaScript front end will always take longer. The downside of Python is that it will consume more computational resources than alternatives like Go or Node.js. I would recommend writing tests and using types, as they help maintenance of bigger projects. CI is easy and can be free nowadays with Github Actions. Deployments should also be automated with Ansible and Docker or similar.</p><h2 id="Analytics-logging-and-monitoring"><a href="#Analytics-logging-and-monitoring" class="headerlink" title="Analytics, logging and monitoring"></a>Analytics, logging and monitoring</h2><p>It is useful to keep track of the application behaviour without having to access the server. For analytics, the most common approach is to use a solution like Google Analytics or the open-source privacy oriented Plausible Analytics. You can also complement these with web server logs.</p><p>For metrics and log analytics, my favorite stack is Elasticsearch+Kibana and Prometheus+Grafana. However, it does not make sense to host these yourself until you grow to a certain size. Elasticsearch is particularly hungry for RAM. I had a good experience with Sematext free tier, which uses the Elastic stack, but opting for Grafana Cloud and Logz.io are also valid approaches. At scale, it is common to have agents like Filebeat to send application logs to Kafka as a buffer, to prevent overloading Elastic and doing additional processing in Logstash. To build status pages I’m using Freshping for HTTP and TCP checks, StatusCake is popular too.</p><h2 id="Scaling-when-traction-is-gained"><a href="#Scaling-when-traction-is-gained" class="headerlink" title="Scaling when traction is gained"></a>Scaling when traction is gained</h2><p>Your project is growing, your data is valuable and downtime is no longer acceptable, now what? At a small scale, configuring individual VMs with Ansible and Docker works well, you can also scale vertically to a certain point. The current state of the art for horizontal scaling and deployment of applications is Kubernetes. The managed cloud offers like AKS from Azure help reduce the cost of ownership by taking care of some operational tasks. It is also possible to host stateful databases in Kubernetes, but managed services are easier. Here’s a diagram of a common N-tier architecture:</p><img src="/2022/The-architecture-of-a-small-side-project/web-architecture.png" class="" title="Web architecture"><p>With a setup like this you can scale well, provided that your application is stateless. The decision of rolling your own or using external services should be considered carefully, to reduce operational burden and keep costs low. On a small scale, 1 or 2 cheap VMs and the free services I suggested will work fine for most projects, but having the knowledge of how to scale is always useful.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;There is a huge amount of people that like to develop side projects. I mostly do it because I want to learn new things, but gaining some </summary>
      
    
    
    
    
    <category term="cloud" scheme="https://briefbytes.com/tags/cloud/"/>
    
    <category term="architecture" scheme="https://briefbytes.com/tags/architecture/"/>
    
  </entry>
  
  <entry>
    <title>Building a full text search engine</title>
    <link href="https://briefbytes.com/2021/Building-a-full-text-search-engine/"/>
    <id>https://briefbytes.com/2021/Building-a-full-text-search-engine/</id>
    <published>2021-07-14T21:15:00.000Z</published>
    <updated>2022-11-16T20:24:29.017Z</updated>
    
    <content type="html"><![CDATA[<p>Information Retrieval (IR) is the activity of finding resources that have relevant information, usually documents, from a large collection, in response to a query. I’ve always been fascinated by large-scale search engines like Google. In this post I will talk a bit about the IR process and how to build a full text search engine in Go. The code is available in <a href="https://github.com/ruial/busca">GitHub</a>.</p><h2 id="Indexing"><a href="#Indexing" class="headerlink" title="Indexing"></a>Indexing</h2><p>Indexing is an operation to facilitate searches, by avoiding the need to do full scans over the entire dataset. With indexes, faster retrievals are traded for additional storage and higher update times because the index has to be maintained. A simple index example in JSON format is presented bellow:</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;inverted-index&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;example&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;doc1&quot;</span><span class="punctuation">,</span> <span class="string">&quot;doc2&quot;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;doc1&quot;</span><span class="punctuation">]</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;documents&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;doc1&quot;</span><span class="punctuation">:</span> <span class="string">&quot;example content&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;doc2&quot;</span><span class="punctuation">:</span> <span class="string">&quot;example&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>As the index stores a list of words and the documents in which they appear, matches are quickly retrieved. Tokenization is the process of separating text into smaller units (words, n-grams, sentences). Other text preprocessing techniques like lower casing, removing stop words, stemming (trimming suffixes), among others, can be useful to improve search results. On <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis.html">Elasticsearch</a>, these text analysis steps are performed by an Analyzer.</p><h2 id="Querying"><a href="#Querying" class="headerlink" title="Querying"></a>Querying</h2><p>An IR process starts when a user enters a query into the system, which is then evaluated, and a set of relevant results are displayed. Query expansion techniques like replacing synonyms and fixing spelling errors are often used to improve relevance. Variations of the <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">tf–idf</a> weighting scheme are able to rank how well each document matches a given query. Other examples of ranking algorithms are Okapi BM25, PageRank and <a href="https://en.wikipedia.org/wiki/Learning_to_rank">machine learning models</a>.</p><p>There are many <a href="https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)">metrics</a> to evaluate search engines. Precision, recall and F1-score are popular when dealing with unordered sets. On huge web search engines, it is not possible to calculate absolute recall as the number of relevant results for a given query is usually unknown, so the mean precision of the top results on a set of queries can be considered. To take order into account there are metrics like the discounted cumulative gain. When an IR system is already deployed, new versions can be evaluated with online metrics and AB testing.</p><h2 id="A-minimal-search-engine-in-Go"><a href="#A-minimal-search-engine-in-Go" class="headerlink" title="A minimal search engine in Go"></a>A minimal search engine in Go</h2><p>Go is a great language to build web services. Its concurrency mechanisms are easy to use, it is statically typed, compiles quickly, dependency management is not a disaster (looking at you, Python) and tooling is also great (race detector, profiler, benchmark tests and no need for test libraries).</p><p>The data structures chosen for the index consist of two maps. One with the terms as keys and document IDs as values, and another with the document IDs as keys and the documents and term frequencies as values. Storing the term frequencies at index time allows faster ranking of search results, as they do not have to be calculated later.</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> DocumentData <span class="keyword">struct</span> &#123;</span><br><span class="line">  Doc         Document</span><br><span class="line">  TermsCount  <span class="type">float64</span></span><br><span class="line">  Frequencies <span class="keyword">map</span>[<span class="type">string</span>]<span class="type">float64</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Index <span class="keyword">struct</span> &#123;</span><br><span class="line">  terms      <span class="keyword">map</span>[<span class="type">string</span>][]core.DocumentID</span><br><span class="line">  documents  <span class="keyword">map</span>[core.DocumentID]core.DocumentData</span><br><span class="line">  analyzer   analysis.Analyzer</span><br><span class="line">  docsMutex  sync.RWMutex</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The RWMutex is important to prevent race conditions. The lock can be held by an arbitrary number of readers or a single writer. To maximize concurrency, the operations done while the lock is held should be fast. I’ve created a snippet with the most important parts:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(i *Index)</span></span> AddDocument(document core.Document) <span class="type">error</span> &#123;</span><br><span class="line">  <span class="comment">// text analysis outside the lock</span></span><br><span class="line">  terms := i.analyzer.Analyze(document.Text())</span><br><span class="line">  frequencies := core.NewTermFrequency(terms)</span><br><span class="line"></span><br><span class="line">  i.docsMutex.Lock()</span><br><span class="line">  <span class="keyword">defer</span> i.docsMutex.Unlock()</span><br><span class="line">  id := document.ID()</span><br><span class="line">  <span class="keyword">if</span> _, ok := i.documents[id]; ok &#123;</span><br><span class="line">    <span class="keyword">return</span> errors.New(<span class="string">&quot;Document already exists&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// update the maps if the document does not already exist</span></span><br><span class="line">  <span class="keyword">var</span> termsCount <span class="type">float64</span></span><br><span class="line">  <span class="keyword">for</span> t, f := <span class="keyword">range</span> frequencies &#123;</span><br><span class="line">    i.terms[t] = <span class="built_in">append</span>(i.terms[t], id)</span><br><span class="line">    termsCount += f</span><br><span class="line">  &#125;</span><br><span class="line">  i.documents[id] = core.DocumentData&#123;Doc: document, Frequencies: frequencies, TermsCount: termsCount&#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(i *Index)</span></span> SearchDocuments(query <span class="type">string</span>, filterFn search.Filter, rankFn search.Ranker) []core.DocumentScore &#123;</span><br><span class="line">  <span class="comment">// use the same analyzer to get search terms, filter the documents and rank them</span></span><br><span class="line">  terms := i.analyzer.Analyze(query)</span><br><span class="line">  docs := i.filterDocuments(terms, filterFn)</span><br><span class="line">  <span class="keyword">return</span> rankFn(terms, docs)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(i *Index)</span></span> filterDocuments(terms []<span class="type">string</span>, filterFn search.Filter) <span class="keyword">map</span>[core.DocumentID]core.DocumentData &#123;</span><br><span class="line">  <span class="comment">// readers must also lock to prevent race conditions</span></span><br><span class="line">  i.docsMutex.RLock()</span><br><span class="line">  <span class="keyword">defer</span> i.docsMutex.RUnlock()</span><br><span class="line">  docs := <span class="built_in">make</span>(<span class="keyword">map</span>[core.DocumentID]core.DocumentData)</span><br><span class="line">  <span class="comment">// filter the documents using the search/document terms and a function that has a condition (AND, OR)</span></span><br><span class="line">  docIds := filterFn(terms, i.terms)</span><br><span class="line">  <span class="keyword">for</span> _, id := <span class="keyword">range</span> docIds &#123;</span><br><span class="line">    docs[id] = i.documents[id]</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> docs</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Usage</span></span><br><span class="line">idx := index.New(index.Opts&#123;Analyzer: analysis.WhitespaceAnalyzer&#123;&#125;&#125;)</span><br><span class="line">idx.AddDocument(core.NewBaseDocument(<span class="string">&quot;doc1&quot;</span>, <span class="string">&quot;some text&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// there are variations to the original tf idf formula, check the code for implementation details</span></span><br><span class="line"><span class="comment">// TfWeightLog - when it is more important to find a term, than the number of times it appears on a document</span></span><br><span class="line"><span class="comment">// IdfWeightSmooth - when a word appears on all documents, idf will be 0, smoothing will cause it to be higher than zero,</span></span><br><span class="line"><span class="comment">// which can be useful when filtering documents and they all contain a term</span></span><br><span class="line">tfidfRanker := search.TfIdfRanker(search.TfWeightLog, search.IdfWeightSmooth)</span><br><span class="line">results := idx.SearchDocuments(<span class="string">&quot;sample query&quot;</span>, search.OrFilter, tfidfRanker)</span><br></pre></td></tr></table></figure><p>After implementing the index data structure and ranking algorithm, it was trivial to add an HTTP API on top of it. However, there are some complex topics like Unicode Text Segmentation, spelling correction (Levenshtein automaton, Norvig algorithm), autocomplete (finite state transducers, n-grams), handling multiple text fields, efficient disk persistence and of course, distributing indexes across different machines for scale (sharding). This is why using an existing solution like Elasticsearch, which is built on top of Lucene, is a good idea. Scalable technologies like Lucene, Cassandra and Kafka use log based data structures, like  LSM-Trees and SSTables, where data is written to segments and periodically merged. If you are interested in distributed systems, I recommend the Designing Data-Intensive Applications <a href="https://www.oreilly.com/library/view/designing-data-intensive-applications/9781491903063">book</a> and Pogrebinsky’s Udemy <a href="https://www.udemy.com/course/distributed-systems-cloud-computing-with-java">course</a>.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Information Retrieval (IR) is the activity of finding resources that have relevant information, usually documents, from a large collectio</summary>
      
    
    
    
    
    <category term="go" scheme="https://briefbytes.com/tags/go/"/>
    
    <category term="information retrieval" scheme="https://briefbytes.com/tags/information-retrieval/"/>
    
    <category term="nlp" scheme="https://briefbytes.com/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>Azure Kubernetes Service Pipelines</title>
    <link href="https://briefbytes.com/2021/Azure-Kubernetes-Service-Pipelines/"/>
    <id>https://briefbytes.com/2021/Azure-Kubernetes-Service-Pipelines/</id>
    <published>2021-06-15T22:00:00.000Z</published>
    <updated>2022-11-16T20:24:29.017Z</updated>
    
    <content type="html"><![CDATA[<p>In the last months I have been working with <a href="https://azure.microsoft.com/en-us/services/devops">Azure DevOps</a> and completed the <a href="https://docs.microsoft.com/en-us/learn/certifications/devops-engineer">Devops Engineer certification</a> from Microsoft. Recently I’ve also been learning Kubernetes and wanted to build an end to end pipeline to deploy a sample Go application, with Ingress and TLS, to the managed <a href="https://azure.microsoft.com/en-us/services/kubernetes-service">Azure Kubernetes Service (AKS)</a>. I will describe the steps I’ve taken and share the GitHub <a href="https://github.com/ruial/aks-devops">repo</a> with all the Infrastructure as Code (IaC) using Terraform.</p><h2 id="Azure-DevOps"><a href="#Azure-DevOps" class="headerlink" title="Azure DevOps"></a>Azure DevOps</h2><p>Azure Devops is Microsoft’s solution to automate the software development lifecycle. It has the following services:</p><ul><li><p><strong>Azure Boards</strong> to manage work items, similar to Jira.</p></li><li><p><strong>Azure Pipelines</strong> for CI&#x2F;CD automation, like Jenkins or GitHub Actions.</p></li><li><p><strong>Azure Repos</strong> is a repository service for Git or TFVC.</p></li><li><p><strong>Azure Test Plans</strong> for manual testing (really expensive)</p></li><li><p><strong>Azure Artifacts</strong> to publish packages, an alternative to Artifactory.</p></li></ul><p>Using a SaaS instead of rolling your own comes at a cost. Pricing is a bit high at 5€&#x2F;user and 12.65€&#x2F;parallel job (for a self hosted agent), with 5 free users and 1 free job. For bigger enterprises, using self hosted agents instead of Microsoft hosted agents is usually the better option as parallel jobs are cheaper, builds should start faster, and you can make use of incremental builds, internal networking and <a href="https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview">managed identities</a>. When choosing Azure Devops instead of other CI&#x2F;CD tools, pricing has to be considered as the free tier limits are reached very soon and pipelines start to queue, however there is less infrastructure and code to manage.</p><h2 id="Kubernetes"><a href="#Kubernetes" class="headerlink" title="Kubernetes"></a>Kubernetes</h2><p>In recent years, <a href="https://kubernetes.io/docs/concepts/overview/what-is-kubernetes">Kubernetes</a> became the standard to manage applications deployed as containers. It simplifies operations like high availability, autoscaling, rolling updates, rollbacks, service discovery, among others. Some of the most important concepts are:</p><ul><li><p><strong>Pods</strong> - groups of containers that share the same compute resources and network. Each pod is scheduled to run on nodes inside the cluster and gets assigned an IP address.</p></li><li><p><strong>Deployments</strong> - describe the desired state of pods, like the number of replicas and rollout strategy.</p></li><li><p><strong>Services</strong> - to expose Pods for consumption inside (ClusterIP) or outside (NodePort, LoadBalancer) the cluster, as a network service.</p></li></ul><p>Kubectl is the command line interface to manage the cluster, which interacts with the Kubernetes API. Deploys can be performed with imperative commands, but declarative yaml manifests are the way to go for a reproducible environment.</p><p>When it comes to exposing services to the outside world, an alternative to using a Service of type LoadBalancer is to combine ClusterIP Services and an Ingress. A LoadBalancer Service per application results in a new public IP and public load balancer on the cloud, which can get expensive. An Ingress, such as <a href="https://kubernetes.github.io/ingress-nginx/deploy/#azure">ingress-nginx</a> is often used to route HTTP and HTTPS traffic to internal services, based on the host header or request path, using a single public IP and cloud Load Balancer. With projects like <a href="https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/azure.md">external-dns</a> and <a href="https://cert-manager.io/docs/installation/kubernetes">cert-manager</a>, the management of DNS records and TLS certificates is fully automated.</p><h2 id="Proof-of-concept"><a href="#Proof-of-concept" class="headerlink" title="Proof of concept"></a>Proof of concept</h2><p>As I was following a course on <a href="https://github.com/stacksimplify/azure-aks-kubernetes-masterclass">Udemy</a>, I’ve adapted some of the examples for a simple use case of deploying a Go application and having the entire infrastructure managed with Terraform. With the <a href="https://registry.terraform.io/providers/microsoft/azuredevops/latest/docs">Azure Devops Provider</a> and <a href="https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs">Azure Provider</a>, I was able to automate most of the provisioning, only had to configure a few things manually in Azure DevOps (add the Kubernetes Environment, create the pipelines) and edit some variables in the pipeline declarations after having the resources provisioned. You would be able to reproduce everything by cloning the repo. I will only describe the global structure, as each file is easy to interpret:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line"># go app with multi-stage container</span><br><span class="line">├── goproject # go app with multi-stage container</span><br><span class="line">│   ├── Dockerfile</span><br><span class="line">│   ├── go.mod</span><br><span class="line">│   ├── main.go</span><br><span class="line">│   └── main_test.go</span><br><span class="line"># manifests for the go app and the ingress, in some orgs these</span><br><span class="line"># could even be managed in different repos by different teams</span><br><span class="line">├── kubernetes</span><br><span class="line">│   ├── gomanifests</span><br><span class="line">│   │   ├── deployment.yml</span><br><span class="line">│   │   └── service.yml</span><br><span class="line">│   └── ingressmanifests</span><br><span class="line">│       ├── azure.json</span><br><span class="line">│       ├── cert-manager-kube-system.yml</span><br><span class="line">│       ├── cert-manager.yml</span><br><span class="line">│       ├── cluster-issuer.yml</span><br><span class="line">│       ├── dns-secret.yml</span><br><span class="line">│       ├── external-dns.yml</span><br><span class="line">│       ├── ingress-nginx.yml</span><br><span class="line">│       └── ingress.yml</span><br><span class="line"># azure devops pipelines definitions</span><br><span class="line">├── pipelines</span><br><span class="line">│   ├── go-master-pipeline.yml</span><br><span class="line">│   └── ingress-pipeline.yml</span><br><span class="line"># terraform configuration files</span><br><span class="line">└── terraform</span><br><span class="line">    ├── ado.tf</span><br><span class="line">    ├── akscluster.tf</span><br><span class="line">    ├── main.tf</span><br><span class="line">    ├── output.tf</span><br><span class="line">    └── variables.tf</span><br></pre></td></tr></table></figure><p>I’ve opted to include the cert-manager and ingress-nginx manifests in the repo but they could be deployed with their GitHub URL or using <a href="https://helm.sh/">Helm</a>. The <em>azure.json</em> file has the details of the managed identity created by AKS, that also has the RBAC assignment to edit DNS records on the Azure DNS zone created by Terraform. To get the required IDs for the config file:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># managed identity client id</span></span><br><span class="line">az aks show -g aks-devops-we -n aks-devops-we-cluster --query <span class="string">&quot;identityProfile.kubeletidentity.clientId&quot;</span></span><br><span class="line"><span class="comment"># subscription id and tenant id</span></span><br><span class="line">az account list</span><br><span class="line"><span class="comment"># encode as base64 the secret azure.json on dns-secret.yml</span></span><br><span class="line"><span class="built_in">cat</span> azure.json | <span class="built_in">base64</span></span><br></pre></td></tr></table></figure><p>After deploying the resources, I’ve created the NS records <em>aks.briefbytes.com</em> pointing to the assigned Azure DNS nameservers to delegate this zone to Azure. A limitation of Azure Devops KubernetesManifest tasks required me to break the cert-manager manifest in 2 because it was not able to deploy Kubernetes objects in 2 different namespaces in the same task.</p><p>Terraform is responsible for creating the Kubernetes cluster, the container registry (ACR) to store images, the public dns zone, role assignments and configuring the project and service connections on Azure Devops. This is achieved by running the following:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this could also be on a pipeline using service principal or managed identity</span></span><br><span class="line"><span class="built_in">export</span> AZDO_PERSONAL_ACCESS_TOKEN=&lt;Generate your PAT with full access permissions&gt;</span><br><span class="line"><span class="built_in">export</span> AZDO_ORG_SERVICE_URL=https://dev.azure.com/&lt;Your org&gt;</span><br><span class="line">az login</span><br><span class="line">terraform plan -out plan</span><br><span class="line">terraform apply plan</span><br><span class="line"></span><br><span class="line"><span class="comment"># to login on the cluster (context saved in ~/.kube/config)</span></span><br><span class="line">az aks get-credentials --resource-group aks-devops-we --name aks-devops-we-cluster</span><br><span class="line">kubectl cluster-info</span><br></pre></td></tr></table></figure><p>Adding an environment on Azure Devops will create another service connection and deployments from different pipelines will be visible there. This must be done manually, as the Terraform provider does not yet support it.</p><img src="/2021/Azure-Kubernetes-Service-Pipelines/aks-env.png" class="" title="Azure Devops AKS Environment"><p>After adding the environment, make sure to create the pipeline and edit the ACR name, its service connection ID and of course set your own DNS zone. Your pipeline should run successfully:</p><img src="/2021/Azure-Kubernetes-Service-Pipelines/go-pipeline-stages.png" class="" title="AKS Pipeline Stages"><p>The creation of DNS records is fast but TLS certificates from <a href="https://letsencrypt.org/">Let’s Encrypt</a> can take a few minutes to be issued. You can use the following commands to inspect the logs:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs -n cert-manager -f &lt;kubectl-pod&gt;</span><br><span class="line">kubectl get certificate</span><br><span class="line">kubectl logs -f $(kubectl get po | egrep -o <span class="string">&#x27;external-dns[A-Za-z0-9-]+&#x27;</span>)</span><br></pre></td></tr></table></figure><p>Take care when changing domain names in production to avoid downtime. Do not forget about readiness and liveness checks on real apps to ensure high availability, as services may take a few seconds on startup and they may become unhealthy because of a bad connection.</p><p>The final result should look like this:</p><img src="/2021/Azure-Kubernetes-Service-Pipelines/aks-result.png" class="" title="AKS Result"><p>With this setup, when code is pushed or merged to the master branch, the pipeline will publish a new image if the unit tests pass and do the rolling update without downtime. To save on AKS costs, do not enable logging as Log Analytics is <a href="https://feedback.azure.com/forums/914020-azure-kubernetes-service-aks/suggestions/38495200-aks-has-a-high-log-analytics-cost-azure-monitor">costly</a>, however in production make sure to setup proper monitoring with Prometheus and logging with the Elastic stack or their alternatives. After testing, the cluster can be destroyed with Terraform or shutdown through Azure CLI:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">az extension add --name aks-preview</span><br><span class="line">az aks stop -g aks-devops-we -n aks-devops-we-cluster</span><br></pre></td></tr></table></figure><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>The cloud and Kubernetes provide massive productivity, scalability and reliability improvements and their adoption will surely continue to grow. CRDs and custom operators enable k8s to manage more than containers, for example you can manage Kafka <a href="https://strimzi.io/docs/operators/latest/overview.html#configuration-points-resources_str">topics</a>. There are so many more topics about Kubernetes and AKS that could be covered but I will finish this post with some useful docs:</p><ul><li><a href="https://kubernetes.io/docs/reference/kubectl/cheatsheet">kubectl Cheat Sheet</a></li><li><a href="https://docs.microsoft.com/en-us/azure/aks/ingress-tls">AKS Ingress</a></li><li><a href="https://docs.microsoft.com/pt-pt/azure/aks/configure-kubenet">AKS Networking</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;In the last months I have been working with &lt;a href=&quot;https://azure.microsoft.com/en-us/services/devops&quot;&gt;Azure DevOps&lt;/a&gt; and completed th</summary>
      
    
    
    
    
    <category term="cloud" scheme="https://briefbytes.com/tags/cloud/"/>
    
    <category term="kubernetes" scheme="https://briefbytes.com/tags/kubernetes/"/>
    
    <category term="devops" scheme="https://briefbytes.com/tags/devops/"/>
    
    <category term="pipelines" scheme="https://briefbytes.com/tags/pipelines/"/>
    
  </entry>
  
  <entry>
    <title>A not so brief DNS introduction</title>
    <link href="https://briefbytes.com/2021/A-not-so-brief-DNS-introduction/"/>
    <id>https://briefbytes.com/2021/A-not-so-brief-DNS-introduction/</id>
    <published>2021-03-26T19:00:00.000Z</published>
    <updated>2022-11-16T20:24:29.017Z</updated>
    
    <content type="html"><![CDATA[<p>The Domain Name System (DNS) is one of the most important pieces of the global internet infrastructure. DNS is an hierarchical and decentralized database system to translate host names into IP addresses. In this post I will share some of my knowledge about DNS.</p><h2 id="Domain-names-and-zones"><a href="#Domain-names-and-zones" class="headerlink" title="Domain names and zones"></a>Domain names and zones</h2><p>To manage a zone in the public internet, we have to buy a domain name like <em>briefbytes.com</em> through a domain registrar. Then we can set the name servers we desire, to manage our DNS records. In my personal case, I’ve chosen Cloudflare for this.</p><p>After having a domain, we can manage the entire zone for that domain. In some companies it is also common to delegate zones. For example, one business unit can manage the root domain and another one could manage all records in the subdomain <em>it.briefbytes.com</em>, using different name servers.</p><p>Most companies will also create their private zones, which are used for internal DNS resolution on <a href="https://tools.ietf.org/html/rfc1918">private computer networks</a>, with private DNS servers. Two popular DNS servers are Bind9 for Linux and Windows DNS Server for Windows Server. Records can be saved in zone files or any other type of database. Cloud providers, such as Azure, also offer managed DNS solutions like <a href="https://docs.microsoft.com/en-us/azure/dns/private-dns-overview">Azure Private DNS</a>, which is capable of creating DNS records for machines as they are provisioned, without having configure <a href="https://tools.ietf.org/html/rfc2136">Dynamic DNS updates</a> using a tool like <em>nsupdate</em>, as is usually done with self-hosted alternatives.</p><h2 id="DNS-records"><a href="#DNS-records" class="headerlink" title="DNS records"></a>DNS records</h2><p>In the DNS protocol, each resource record must have a name, a type, a time to live (TTL) for caching purposes and some data with the format dependant on the type. More details can be found in <a href="https://tools.ietf.org/html/rfc1035">RFC-1035</a>. The main record types are:</p><ul><li><p>A (IPv4 host address)</p></li><li><p>AAAA (IPv6 host address)</p></li><li><p>CNAME (ALIAS for host address)</p></li><li><p>SOA (Start of Authority)</p></li><li><p>NS (Name Servers)</p></li><li><p>MX (Mail Servers)</p></li><li><p>SRV (Service records for <a href="https://tools.ietf.org/html/rfc2782">service discovery</a>, however check out <a href="https://www.consul.io/docs/discovery/dns">Consul</a> and <a href="https://github.com/hashicorp/consul-template">consul-template</a> they are really awesome for service discovery and work with DNS)</p></li><li><p>TXT (Arbitrary text strings, useful for proving domain ownership)</p></li><li><p>PTR (Reverse lookup, translates IP to name)</p></li></ul><p>There are some peculiarities about different record types. For example, a CNAME record cannot coexist with other records with the same name, which is why it cannot be created in the apex zone (which is often represented by the name ‘@’). TXT records are the only ones that can contain multiple values in a single resource record, although it is more common to create multiple records with a single value. PTR records are part of the reverse lookup zone, not the forward lookup zone like the rest. As such, they cannot be changed with your DNS provider because they are managed by the owners of the IP addresses, so they can only be changed by the respective internet service provider (ISP) or cloud provider, unless it’s your private zone. There’s also wildcard records and glue records.</p><p>When two resource records are created with the same name and type, but different data, they are referred to as resource record sets (<a href="https://tools.ietf.org/html/rfc2181">RRSet</a>). CNAMEs and SOA are exceptions and cannot exist in sets. RRSets enable round-robin load balancing through DNS by rotating the order in which the list of record values is returned, as the first one is often used by clients. However, this is a poor man’s load balancer because there is no guarantee that the load will be evenly distributed and if a server goes down, the IP will still be cached until the TTL expires. Round-robin DNS is not an alternative to actual L4 or L7 load balancers, which perform health checks and rotation.</p><p>The alternative to RRSets for load balancing at a global level is Anycast. With Anycast, ISPs and cloud providers, using BGP, can publish different routes in different parts of the world to reach certain IPs, making it possible to have multiple machines or network appliances, like load balancers, around the world with the same IP. With Anycast, it is possible to use a solution like <a href="https://docs.microsoft.com/en-us/azure/traffic-manager/traffic-manager-routing-methods">Azure Traffic Manager</a> to return different DNS responses based on user locations (actually, based on their DNS servers locations, which are also spread across the world using Anycast, so even the single Google DNS IP 8.8.8.8 maps to many servers). If you use <em>dig</em> or <em>nslookup</em> on the Google domain you will get different answers on different countries.</p><h2 id="How-DNS-queries-are-made"><a href="#How-DNS-queries-are-made" class="headerlink" title="How DNS queries are made"></a>How DNS queries are made</h2><p>When trying to resolve a domain like <em>briefbytes.com</em>, the operating system does the following operations to retrieve the associated IP:</p><ol><li><p>Check the hosts file (<em>&#x2F;etc&#x2F;hosts</em> on Linux and Mac OS, inside System32 folder on Windows)</p></li><li><p>Check the system DNS cache (both Windows and Linux have a local DNS cache, however Linux does not, although a local caching DNS server like dnsmasq or systemd-resolved can be installed)</p></li><li><p>Send the query to the configured name servers, which by default is set through DHCP (even on Azure virtual networks), but can be changed (<em>&#x2F;etc&#x2F;resolv.conf</em> on Linux and Mac OS, network settings on Windows)</p></li></ol><p>When a query is received by a name server, in the case of the Windows DNS server, a widely used DNS server in many enterprises, the following happens:</p><ol><li><p>Check the local cache</p></li><li><p>If the server is authoritative for the zone, return authoritative answer</p></li><li><p>Do a recursive query through a conditional forwarder if a name server is set for a certain domain</p></li><li><p>Do a recursive query through a forwarder if a name server is configured (like 8.8.8.8 for Google DNS or 168.63.129.16 in Azure networks)</p></li><li><p>Do iterative queries on the root name servers and follow referrals, if a forwarder is not set or fails (configurable)</p></li></ol><p>A fully qualified domain names (FQDN) always has a dot at the end to avoid issues with relative names (e.g.: <code>www.briefbytes.com.</code>). Here’s one image from the <a href="https://docs.microsoft.com/en-us/windows-server/identity/ad-ds/plan/reviewing-dns-concepts">Microsoft docs</a>, which explains name resolution very nicely:</p><img src="/2021/A-not-so-brief-DNS-introduction/dns.gif" class="" title="DNS resolution"><p>A recursive query from the root name servers, with the objective of getting an authoritative answer for this blog domain can be done like this:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">nslookup -<span class="built_in">type</span>=ns com a.root-servers.net</span><br><span class="line">nslookup -<span class="built_in">type</span>=ns briefbytes.com a.gtld-servers.net</span><br><span class="line"><span class="comment"># authoritative answer</span></span><br><span class="line">nslookup briefbytes.com ned.ns.cloudflare.com</span><br><span class="line"></span><br><span class="line"><span class="comment"># non authoritative</span></span><br><span class="line">nslookup -debug briefbytes.com 8.8.8.8</span><br><span class="line">  Non-authoritative answer:</span><br><span class="line">  Name:briefbytes.com</span><br><span class="line">  Address: 172.67.165.65</span><br><span class="line">  Name:briefbytes.com</span><br><span class="line">  Address: 104.21.57.175</span><br><span class="line">  Name:briefbytes.com</span><br><span class="line">  Address: 2606:4700:3033::6815:39af</span><br><span class="line">  Name:briefbytes.com</span><br><span class="line">  Address: 2606:4700:3034::ac43:a541</span><br></pre></td></tr></table></figure><p>There are actually 2 name servers configured for my domain in Cloudflare, with some friendly names. Ned is the primary name server and Tani is a secondary name server. Secondary zones are read only copies of the primary (in some cases like Windows DNS server, all domain controllers are actually primary servers, since writes can happen in any server and data is saved to Active Directory and replicated). By doing a SOA query on each name server we can see a serial number. This value is incremented on the primary server when updates are made. The secondary server periodically checks the serial number and does an incremental <a href="https://tools.ietf.org/html/rfc5936">zone transfer</a> when needed, to sync the records. DNS protocol supports TCP, although UDP is preferred for small requests (up to 512 bytes).</p><h2 id="Security-and-privacy"><a href="#Security-and-privacy" class="headerlink" title="Security and privacy"></a>Security and privacy</h2><p>DNS traffic is not encrypted, so even if you change your default name servers provided by your ISP, they are able to know which sites you visit based on your DNS queries. In my country, ISPs are forced by the government censorship to block certain domain names like LibGen, so changing them is essential for many people. Let’s hope they never decide to block IP address ranges and take down entire regions of the internet.</p><p>To prevent ISPs from snooping our traffic to sell our data, DNS over HTTPS (<a href="https://tools.ietf.org/html/rfc8484">DoH</a>) seems to be the future. However, keep in mind that the actual DNS servers will still have the data. Browser vendors like <a href="https://blog.mozilla.org/blog/2020/02/25/firefox-continues-push-to-bring-dns-over-https-by-default-for-us-users">Firefox</a> pushing DoH, however DNS should be a global operating system setting and not set by individual browsers or other applications. I want to set <a href="https://pi-hole.net/">Pi-Hole</a> as a DNS server to block ads on the network and propagate it to all machines via router’s DHCP without additional configuration, this is what standards are for.</p><p>Regarding DNS security there’s also <a href="https://tools.ietf.org/html/rfc4033">DNSSEC</a> to verify the integrity of DNS records but it is complex and not widely used. To authenticate updates and zone transfers on a DNS database there’s <a href="https://tools.ietf.org/html/rfc2845">TSIG</a>, which also guarantees message integrity using shared secrets.</p><p>With DNS management comes public key infrastructure (<a href="https://tools.ietf.org/html/rfc2510">PKI</a>) certificate management, as we want to issue certificates for domain names. On public DNS zones in the internet, we have to pay for the certificates, unless we use <a href="https://letsencrypt.org/">Let’s Encrypt</a>, as they must be signed by a trusted certificate authority (CA). For private DNS zones on our network, we can create a new internal CA and add the root certificate on all the machines we want the issued certificates to be trusted. To avoid managing internal CAs, some companies buy wildcard certificates and install them with configuration management tools on the web servers and any other computer should trust these certificates without additional work, as the root CA is already trusted by the systems.</p><p>The DNS protocol is a critical part of any infrastructure. Even the machines that have to be locked down for compliance reasons will often have DNS traffic enabled, which can be used for <a href="https://blogs.akamai.com/2017/09/introduction-to-dns-data-exfiltration.html">data exfiltration</a>. Using DNS for this is far from ideal, as each outbound lookup query will have a maximum of 255 bytes and order of arrival is unpredictable. From a security perspective, it makes sense to monitor DNS traffic. When exposing a DNS server on the internet, recursive queries should be disabled, so that only authoritative answers are returned. For an interesting read, check this <a href="https://igor-blue.github.io/2021/03/24/apt1.html">article</a> where attackers were able to infect a CI&#x2F;CD system and tamper with the generated binaries, however they used HTTPS to exfiltrate data.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>This was a not so brief introduction to DNS. One last advice is if you ever need to develop software related with DNS (at work I created an API to manage DNS in multiple providers, including access control, which enabled DNS as code and k8s ExternalDNS integration with existing infrastructure), check the following Go libraries:</p><ul><li><p><a href="https://github.com/miekg/dns">https://github.com/miekg/dns</a> - to create DNS clients or servers</p></li><li><p><a href="https://github.com/bodgit/tsig">https://github.com/bodgit/tsig</a> - perform dynamic updates with <a href="https://www.ietf.org/rfc/rfc3645.txt">GSS-TSIG</a> on Windows Server with ‘secure only’ updates</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;The Domain Name System (DNS) is one of the most important pieces of the global internet infrastructure. DNS is an hierarchical and decent</summary>
      
    
    
    
    
    <category term="cloud" scheme="https://briefbytes.com/tags/cloud/"/>
    
    <category term="dns" scheme="https://briefbytes.com/tags/dns/"/>
    
    <category term="infrastructure" scheme="https://briefbytes.com/tags/infrastructure/"/>
    
  </entry>
  
  <entry>
    <title>Secure Prometheus with OIDC or LDAP</title>
    <link href="https://briefbytes.com/2020/Secure-Prometheus-with-OIDC-or-LDAP/"/>
    <id>https://briefbytes.com/2020/Secure-Prometheus-with-OIDC-or-LDAP/</id>
    <published>2020-11-28T22:42:50.000Z</published>
    <updated>2022-11-16T20:24:29.025Z</updated>
    
    <content type="html"><![CDATA[<p>When hosting applications for internal use, it is common to only allow connections from private networks. However, in addition to using a VPN, being able to authenticate and authorize users with a centralized identity system is often required for security reasons. I found two good alternatives that I will describe next, along with a monitoring use case.</p><h2 id="Prometheus-and-Grafana"><a href="#Prometheus-and-Grafana" class="headerlink" title="Prometheus and Grafana"></a>Prometheus and Grafana</h2><p><a href="https://prometheus.io/">Prometheus</a> is one of the most widely used systems for monitoring and alerting, while <a href="https://grafana.com/">Grafana</a> is very popular to create observability dashboards like this one:</p><img src="/2020/Secure-Prometheus-with-OIDC-or-LDAP/grafana.png" class="" title="Grafana dashboard"><p>Although some applications like Grafana already have support for OAuth or LDAP authentication, others like Prometheus delegate this responsibility to other systems.</p><h2 id="Pomerium"><a href="#Pomerium" class="headerlink" title="Pomerium"></a>Pomerium</h2><p><a href="https://www.pomerium.com/docs">Pomerium</a> is an identity-aware proxy developed in Go. It can be placed in front of internal services and integrate with identity providers using <a href="https://en.wikipedia.org/wiki/OpenID_Connect">OpenID Connect</a>. Out of the box, it supports identity providers like Google, Github and standard compliant OIDC providers like <a href="https://github.com/panva/node-oidc-provider">node-oidc-provider</a> or <a href="https://identityserver4.readthedocs.io/en/latest">IdentityServer</a>. Here’s a comprehensive <a href="https://www.scottbrady91.com/OpenID-Connect/Getting-Started-with-oidc-provider">guide</a> to setup a provider.</p><h2 id="Authelia"><a href="#Authelia" class="headerlink" title="Authelia"></a>Authelia</h2><p><a href="https://www.authelia.com/docs">Authelia</a> is an authentication and authorization server written in Go. It requires integration with a reverse proxy, such as Nginx. Instead of OIDC, LDAP is supported so it can be integrated with Active Directory (AD). On my tests I’ve used a local users file and Nginx, as it was the simplest approach. I also had success with AD and HAProxy (with LUA plugins), following the official documentation.</p><h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p>Both auth technologies work quite well. It boils down to what you need to integrate with. Some additional features can be added on the reverse proxy or load balancer, like rate limiting or modifying request headers.</p><p>I’ve setup a <a href="https://github.com/ruial/monitoring-auth-demo">demo project</a> to demonstrate their capabilities that uses the following architecture:</p><img src="/2020/Secure-Prometheus-with-OIDC-or-LDAP/auth.png" class="" title="Auth flow"><p>Besides proxy mode, which passes all traffic through, Pomerium also supports Forward authentication, which causes the reverse proxy to call an endpoint to verify if the user is authorized. User information is then placed on request headers. Both technologies make use of cookie sessions and redirect the user to the login page when not authenticated. They also support <a href="https://redis.io/">Redis</a>, which is important to preserve sessions during restarts or if using multiple instances, as you don’t usually want to depend on sticky sessions at the load balancer level.</p><p>If you are interested in authenticating requests from public internet Github actions runners with internal services inside a private subnet, check out this <a href="https://github.com/ruial/actions-oidc-proxy">OIDC proxy</a> approach.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;When hosting applications for internal use, it is common to only allow connections from private networks. However, in addition to using a</summary>
      
    
    
    
    
    <category term="prometheus" scheme="https://briefbytes.com/tags/prometheus/"/>
    
    <category term="ldap" scheme="https://briefbytes.com/tags/ldap/"/>
    
    <category term="oidc" scheme="https://briefbytes.com/tags/oidc/"/>
    
  </entry>
  
  <entry>
    <title>OpenVPN deployment with Terraform and Ansible</title>
    <link href="https://briefbytes.com/2020/OpenVPN-deployment-with-Terraform-and-Ansible/"/>
    <id>https://briefbytes.com/2020/OpenVPN-deployment-with-Terraform-and-Ansible/</id>
    <published>2020-09-24T20:00:00.000Z</published>
    <updated>2022-11-16T20:24:29.025Z</updated>
    
    <content type="html"><![CDATA[<p>As the number of machines and services that you manage increases, manually creating and configuring your infrastructure is not scalable. As such, provisioning and configuration management tools, such as <a href="https://www.terraform.io/">Terraform</a> and <a href="https://www.ansible.com/">Ansible</a> are extremely useful. I’m going to show how to automate the deployment of <a href="https://openvpn.net/">OpenVPN</a> on <a href="https://azure.microsoft.com/">Azure</a>.</p><h2 id="OpenVPN"><a href="#OpenVPN" class="headerlink" title="OpenVPN"></a>OpenVPN</h2><p>Occasionally, I want to mask my IP address to unlock geo-restricted content. Instead of paying a monthly fee to a VPN provider, a viable alternative is to create a temporary virtual machine on a cloud provider, install OpenVPN, and then destroy it, which is cheap. With automation, this process is very fast. If you want to learn more about OpenVPN and Public Key Infrastructure (PKI), check this <a href="https://www.packtpub.com/product/mastering-openvpn/9781783553136">book</a>. For production use, there are additional security steps that should be taken, especially on the PKI side.</p><p>You can follow these instructions to deploy your own OpenVPN server:</p><ol><li>Clone the <a href="https://github.com/ruial/openvpn-demo">GitHub repository</a></li><li>Generate an SSH key (<em>ssh-keygen</em>) or set an admin password for the server</li><li>Create infrastructure using Terraform (check README)</li><li>Edit the inventory.ini or your <em>&#x2F;etc&#x2F;hosts</em> to include the server IP</li><li>Run the Ansible playbook (check README)</li><li>Start OpenVPN client (<em>openvpn –config playbooks&#x2F;out&#x2F;client-host.conf</em>)</li><li>Destroy infrastructure when done (check README)</li></ol><h2 id="Demo-project-structure"><a href="#Demo-project-structure" class="headerlink" title="Demo project structure"></a>Demo project structure</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">├── playbooks</span><br><span class="line">│   ├── ansible.cfg         - ansible configuration file</span><br><span class="line">│   ├── handlers            - handlers for events</span><br><span class="line">│   ├── inventory.ini       - inventory of servers</span><br><span class="line">│   ├── main.yml            - main playbook (the entrypoint)</span><br><span class="line">│   ├── out                 - output folder for client config</span><br><span class="line">│   ├── tasks</span><br><span class="line">│   │   ├── easyrsa.yml     - easyrsa config tasks</span><br><span class="line">│   │   └── openvpn.yml     - openvpn config tasks</span><br><span class="line">│   └── templates</span><br><span class="line">│       ├── client.conf.j2  - openvpn client config template</span><br><span class="line">│       └── server.conf.j2  - openvpn server config template</span><br><span class="line">├── provision</span><br><span class="line">│   ├── instance.tf         - virtual machine and network association</span><br><span class="line">│   ├── main.tf             - provider definition and resource group</span><br><span class="line">│   ├── network.tf          - virtual network, subnet and security group</span><br><span class="line">│   ├── output.tf           - output variables (IP address)</span><br><span class="line">│   ├── terraform.tfstate   - automatically generated tfstate</span><br><span class="line">│   └── vars.tf             - input variables that can be overridden</span><br><span class="line">└── readme.md               - project info and instructions</span><br></pre></td></tr></table></figure><h2 id="Terraform"><a href="#Terraform" class="headerlink" title="Terraform"></a>Terraform</h2><p>Terraform is the most popular tool to provision resources on public clouds. The infrastructure is defined with a declarative domain-specific-language (DSL) called HashiCorp Configuration Language (HCL). It saves the infrastructure state in a local file, provides locking and allows to plan the changes before applying them. When working with multiple people, <a href="https://www.terraform.io/docs/state/remote.html">remote state</a> should be used to prevent concurrent runs.</p><p>As each cloud provider is different, there are multiple terraform <a href="https://www.terraform.io/docs/providers/azurerm">providers</a>. With Terraform we can keep the entire infrastructure as code and others can review changes, which helps reduce human errors. Initial configurations of the operating system (OS) can be specified, which is helpful. Many companies choose to build their own OS image. Here’s a sample of a terraform config file:</p><figure class="highlight python"><figcaption><span>instance.tf</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">resource <span class="string">&quot;azurerm_virtual_machine&quot;</span> <span class="string">&quot;demo-instance&quot;</span> &#123;</span><br><span class="line">  name                  = <span class="string">&quot;$&#123;var.prefix&#125;-vm&quot;</span></span><br><span class="line">  location              = var.location</span><br><span class="line">  resource_group_name   = azurerm_resource_group.demo.name</span><br><span class="line">  network_interface_ids = [azurerm_network_interface.demo-instance.<span class="built_in">id</span>]</span><br><span class="line">  vm_size               = <span class="string">&quot;Standard_B1ls&quot;</span></span><br><span class="line"></span><br><span class="line">  delete_os_disk_on_termination = true</span><br><span class="line">  delete_data_disks_on_termination = true</span><br><span class="line"></span><br><span class="line">  storage_image_reference &#123;</span><br><span class="line">    publisher = <span class="string">&quot;OpenLogic&quot;</span></span><br><span class="line">    offer     = <span class="string">&quot;CentOS&quot;</span></span><br><span class="line">    sku       = <span class="string">&quot;7.7&quot;</span></span><br><span class="line">    version   = <span class="string">&quot;latest&quot;</span></span><br><span class="line">  &#125;</span><br><span class="line">  storage_os_disk &#123;</span><br><span class="line">    name              = <span class="string">&quot;$&#123;var.prefix&#125;-osdisk1&quot;</span></span><br><span class="line">    caching           = <span class="string">&quot;ReadWrite&quot;</span></span><br><span class="line">    create_option     = <span class="string">&quot;FromImage&quot;</span></span><br><span class="line">    managed_disk_type = <span class="string">&quot;Standard_LRS&quot;</span></span><br><span class="line">  &#125;</span><br><span class="line">  os_profile &#123;</span><br><span class="line">    computer_name  = <span class="string">&quot;demo-instance&quot;</span></span><br><span class="line">    admin_username = <span class="string">&quot;demo&quot;</span></span><br><span class="line">    <span class="comment">#admin_password = &quot;...&quot;</span></span><br><span class="line">  &#125;</span><br><span class="line">  os_profile_linux_config &#123;</span><br><span class="line">    disable_password_authentication = true</span><br><span class="line">    ssh_keys &#123;</span><br><span class="line">      key_data = file(<span class="string">&quot;~/.ssh/id_rsa.pub&quot;</span>)</span><br><span class="line">      path     = <span class="string">&quot;/home/demo/.ssh/authorized_keys&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h2 id="Ansible"><a href="#Ansible" class="headerlink" title="Ansible"></a>Ansible</h2><p>Ansible is an agentless configuration management tool, which uses SSH to push and execute playbooks. Playbooks contain sequences of tasks and should be idempotent. For reusable code check <a href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_reuse_roles.html">Ansible Roles</a>. I’ve developed a playbook to install OpenVPN on CentOS:</p><figure class="highlight yaml"><figcaption><span>main.yml</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">hosts:</span> <span class="string">openvpn</span></span><br><span class="line">  <span class="attr">become:</span> <span class="literal">yes</span></span><br><span class="line"></span><br><span class="line">  <span class="attr">tasks:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">install</span> <span class="string">packages</span></span><br><span class="line">    <span class="attr">package:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">&quot;<span class="template-variable">&#123;&#123; item &#125;&#125;</span>&quot;</span></span><br><span class="line">      <span class="attr">state:</span> <span class="string">present</span></span><br><span class="line">    <span class="attr">loop:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">epel-release</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">firewalld</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">easy-rsa</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">openvpn</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">easy-rsa</span></span><br><span class="line">    <span class="attr">include_tasks:</span> <span class="string">tasks/easyrsa.yml</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">openvpn</span></span><br><span class="line">    <span class="attr">include_tasks:</span> <span class="string">tasks/openvpn.yml</span></span><br><span class="line"></span><br><span class="line">  <span class="attr">handlers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">include:</span> <span class="string">handlers/main.yml</span></span><br></pre></td></tr></table></figure><p>I didn’t include all the tasks here because they would take too much space. At first, some required packages are installed and then <em>easy-rsa</em> is used to generate server and client certificates. Then, OpenVPN and the firewall are configured. Lastly, any changes to the server config will trigger a service reload and with the client config automatically transferred, you are able to use your VPN. It is also possible to configure OpenVPN to use your own DNS server if you want to do DNS analytics or block ads with Pi-Hole, which is what I’ve done with <a href="https://www.producthunt.com/posts/ad-pruner">Ad Pruner</a>. Another cool use case is to setup port forwarding (with additional iptables rules), as an alternative to ngrok. If you want to better organize your IaaC and automate provisioning I can recommend <a href="https://terragrunt.gruntwork.io/">Terragrunt</a> and <a href="https://www.runatlantis.io/">Atlantis</a>.</p><h2 id="Closing-thoughts"><a href="#Closing-thoughts" class="headerlink" title="Closing thoughts"></a>Closing thoughts</h2><p>There is a lot to say about infrastructure and application deployment. Immutable vs mutable infrastructure, push vs pull configuration management, kubernetes and containers vs virtual machines, cloud managed services vs self hosted, but I leave all these for another day.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;As the number of machines and services that you manage increases, manually creating and configuring your infrastructure is not scalable. </summary>
      
    
    
    
    
    <category term="automation" scheme="https://briefbytes.com/tags/automation/"/>
    
    <category term="ansible" scheme="https://briefbytes.com/tags/ansible/"/>
    
    <category term="terraform" scheme="https://briefbytes.com/tags/terraform/"/>
    
  </entry>
  
  <entry>
    <title>Machine learning basics</title>
    <link href="https://briefbytes.com/2020/Machine-learning-basics/"/>
    <id>https://briefbytes.com/2020/Machine-learning-basics/</id>
    <published>2020-02-08T15:38:06.000Z</published>
    <updated>2022-11-16T20:24:29.021Z</updated>
    
    <content type="html"><![CDATA[<p>After my last post on <a href="/2019/Extracting-and-analysing-data-using-R/" title="data analysis">data analysis</a>, I will now briefly describe how to achieve a good score for the Titanic survival classification, the house prices regression, and the NLP twitter disaster problems in Kaggle. The <a href="https://topepo.github.io/caret/available-models.html">caret</a> library in R makes it easy to test different machine learning algorithms.</p><h2 id="Machine-learning"><a href="#Machine-learning" class="headerlink" title="Machine learning"></a>Machine learning</h2><p>Machine learning algorithms are able to learn from past data how to make predictions on new data. The most common types of ML algorithms are supervised or unsupervised.</p><p>Supervised algorithms are used to solve classification (predict a class) and regression (predict continuous variable) problems. Training data contains labels and common metrics to evaluate models are accuracy, precision and recall for classification, or root mean square error (RMSE) and R-squared (R2) for regression. Prediction of time series can be made using other type of statistical models, like ARIMA and <a href="https://facebook.github.io/prophet">prophet</a>.</p><p>Unsupervised algorithms usually consist on dimensionality reduction (PCA, SVD, t-sne), clustering (hierarchical, k-means, DBSCAN), anomaly detection and a few others. These do not require training data, and there are different techniques to evaluate each algorithm.</p><h2 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h2><p>The Titanic problem is a binary classification problem that consists on predicting the passengers who survived the disaster. Let’s take a look at the data.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt; glimpse(titanic_train)</span><br><span class="line">Observations: 891</span><br><span class="line">Variables: 12</span><br><span class="line">$ PassengerId &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 2...</span><br><span class="line">$ Survived    &lt;int&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, ...</span><br><span class="line">$ Pclass      &lt;int&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, ...</span><br><span class="line">$ Name        &lt;fct&gt; &quot;Braund, Mr. Owen Harris&quot;, &quot;Cumings, Mrs. John Bradley (Florence Briggs Thayer)&quot;, &quot;Heikkinen, Mi...</span><br><span class="line">$ Sex         &lt;fct&gt; male, female, female, female, male, male, male, male, female, female, female, female, male, male...</span><br><span class="line">$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, 55, 2, NA, 31, NA, 35, 34, 15, 28, 8, ...</span><br><span class="line">$ SibSp       &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0, 1, ...</span><br><span class="line">$ Parch       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 2, 0, 0, 0, 0, ...</span><br><span class="line">$ Ticket      &lt;fct&gt; A/5 21171, PC 17599, STON/O2. 3101282, 113803, 373450, 330877, 17463, 349909, 347742, 237736, PP...</span><br><span class="line">$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.7000, 2...</span><br><span class="line">$ Cabin       &lt;fct&gt; , C85, , C123, , , E46, , , , G6, C103, , , , , , , , , , D56, , A6, , , , C23 C25 C27, , , , B7...</span><br><span class="line">$ Embarked    &lt;fct&gt; S, C, S, S, S, Q, S, S, S, C, S, S, S, S, S, S, Q, S, S, C, S, S, Q, S, S, S, C, S, Q, S, C, C, ...</span><br></pre></td></tr></table></figure><p>Feature engineering can be more important than the machine learning algorithms applied. Some features like Cabin contain too many missing values and should be dropped. Other features may added, like the Title (Mr, Ms) from the name, or the Fsize (family size) taking into account the SibSp (siblings&#x2F;spouses), Parch (parents&#x2F;children) and Ticket (number of duplicate tickets). Any missing values in the observations must be imputed.</p><p>As a baseline, since most people died, a naive algorithm could predict that all people died and would achieve 62% accuracy on the train set. A smarter baseline model would predict that all men died and women survived, which would result in 79% accuracy. A machine learning algorithm should recognize patterns in the data and have better accuracy. Cross-validation (CV) is useful to prevent models from overfitting (achieving high train accuracy but low test accuracy).</p><p>I’ve tried logistic regression, random forest, gradient boosting, however the algorithm that achieved best results in Kaggle was a simple tree classifier with 83% CV accuracy and 80% accuracy in the test set. The smart baseline would result in 77% test accuracy, so a small improvement was made.</p><img src="/2020/Machine-learning-basics/tree.png" class="" title="Decision tree"><p>With this decision tree, the Title, Pclass, Fsize, Sex and Embarked are the features used to decide the fate of a passenger. A simple visualization of the train data shows hints on why this tree was made.</p><img src="/2020/Machine-learning-basics/titanic-data.png" class="" title="Titanic data"><p>The tree is very interpretable. It captures that on Pclass 3, big families die, as well as Misses who embarked on S. For Pclass 1 and 2 it predicts that children and women live.</p><h2 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a>Regression</h2><p>For this problem the predicted variable, Price, is skewed to the right, so a log transformation is helpful to make the distribution more normal and improve the predictions. The train dataset contains 1460 observations and 81 variables, most of which are useless.</p><p>A lot of feature engineering was done to improve the predictions. A few variables have missing values but can be imputed with related variables. Right skewed variables should be transformed using a log function. Encoding a few categorical values as ordinals and removing outliers also improves results. A correlation matrix is helpful to select the most important features for models that are sensitive to multicollinearity, like linear regression.</p><img src="/2020/Machine-learning-basics/correlation.png" class="" title="Correlation matrix"><p>Lasso regression achieved 0.11183 CV RMSE and 0.11911 test RMSE. Elasticnet was close behind, while ridge&#x2F;linear regression and gradient boosting were worst but still an improvement over a mean baseline.</p><p>To improve results I used k-means clustering to create 2 clusters and train a lasso model for each cluster. With this approach, the cluster for each test observation had to be predicted. Performance improved to 0.11897 RMSE. I have seen this cluster and then predict with linear models (not really needed for tree based models) approach in the EDX MOOC called The Analytics Edge, which I do recommend as it was very practical, easy to follow, covered many topics and included many examples and exercises.</p><img src="/2020/Machine-learning-basics/clusters.png" class="" title="Clusters"><p>With a weighted combination of all models, a final 0.11765 RMSE was reached, which at the time was enough to be in the top 20%. To further improve results more feature engineering and hyperparameter tuning would be required.</p><h2 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h2><p>For this natural language problem I have used the R <a href="https://quanteda.io/">quanteda</a> library, which contains many helpful methods for text mining. Besides statistical analysis, NLP techniques like stemming and lemmatization can be used. One of the most helpful ways to compare the most used words in tweets that have been labeled as disaster or not is to plot a wordcloud.</p><img src="/2020/Machine-learning-basics/wordcloud.png" class="" title="Wordcloud"><p>The data was not clean, there were duplicate tweets with different labels, so the best approach is to use the mode to select the right label. To try a ML algorithm, text must be transformed to tabular data, where each row is a tweet, or document, and each column is a word, or feature. For the feature value I had best results with the word count, but binary encoding or tf-idf are also valid choices. Additional features like number of URLs and # also improved model performance.</p><p>Since the resulting matrix is sparse and has many dimensions, it is helpful to remove words&#x2F;columns that rarely appear. Naive bayes and Linear SVM algorithms are often used for text classification because they are fast to train on high dimensional data and provide good results. For more demanding models to train faster, a dimensionality reduction algorithm (e.g.: LDA, LSA) is typically applied first. With SVM I was able to reach 83% CV accuracy and 80% test accuracy. Bigrams did not improve the model but ensembles probably could. Using a part of speech <a href="https://spacyr.quanteda.io/articles/using_spacyr.html#tokenizing-and-tagging-texts">tagger</a> and filtering word classes like nouns&#x2F;verbs can <a href="https://bnosac.github.io/udpipe/docs/doc6.html#basic-topic-modelling">help</a>.</p><p>Unsupervised techniques to generate dense word embeddings like <a href="https://spark.apache.org/docs/latest/ml-features.html#word2vec">word2vec</a>, based on neural networks, and <a href="https://nlp.stanford.edu/projects/glove">GloVe</a>, based on word co-occurrence, or even simpler <a href="https://www.r-bloggers.com/2020/08/whats-the-difference-between-instagram-and-tiktok-using-word-embeddings-to-find-out-2/">PMI+SVD</a> approaches are quite interesting. Words used in similar contexts will be closer in the vector space and documents are compared with <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/dense-vector.html#index-vectors-knn-search">cosine similarity</a> by averaging the word vectors. Using these vectors, documents can be searched using approximate k-NN methods. Recently, other techniques&#x2F;models have achieved good results in some tasks, such as <a href="https://huggingface.co/docs/transformers/model_doc/bert">BERT</a>, which captures contextual embeddings, <a href="https://fasttext.cc/">fastText</a>, that learns vectors from character n-grams, <a href="https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html">doc2vec</a>, <a href="https://cran.r-project.org/web/packages/fastai/vignettes/gpt.html">GPT-2</a>, and the list goes on.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>I am satisfied with the results obtained and was able to learn more about machine learning. I did not explain everything and did not include any code on this post, feel free to check the code on <a href="https://github.com/ruial/kaggle-problems">github</a> which contains more comments. R is fine for data analysis, but I would recommend Python if you want to deploy your ML workloads in production and if you want to use state of the art NLP models.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;After my last post on &lt;a href=&quot;/2019/Extracting-and-analysing-data-using-R/&quot; title=&quot;data analysis&quot;&gt;data analysis&lt;/a&gt;, I will now briefly </summary>
      
    
    
    
    
    <category term="data" scheme="https://briefbytes.com/tags/data/"/>
    
    <category term="R" scheme="https://briefbytes.com/tags/R/"/>
    
  </entry>
  
  <entry>
    <title>Query MongoDB using SQL with Presto</title>
    <link href="https://briefbytes.com/2019/Query-MongoDB-using-SQL-with-Presto/"/>
    <id>https://briefbytes.com/2019/Query-MongoDB-using-SQL-with-Presto/</id>
    <published>2019-11-17T17:00:00.000Z</published>
    <updated>2022-11-16T20:24:29.025Z</updated>
    
    <content type="html"><![CDATA[<p>With an increasing number of specialized databases, each having their own query languages, data analysts have a hard time to combine data from multiples sources. To mitigate this issue, Facebook created <a href="https://prestosql.io/">Presto</a>, a high performance, distributed SQL query engine for big data. I will be creating a small simulation using <a href="https://www.metabase.com/">Metabase</a>, a web based open-source BI solution to visualize data from <a href="https://www.mongodb.com/">MongoDB</a>.</p><h2 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h2><p><a href="https://www.docker.com/">Docker</a> is a container platform. Containers package software applications and all their dependencies, which helps enable reproducible infrastructure. <a href="https://hub.docker.com/">Docker Hub</a> contains many container images so we have a starting point and don’t have to package everything ourselves. This is extremely helpful, specially during development.</p><p>Each container should only have one function. For example, a simple scenario when doing back-end development is to use a container for the application and another for the database. To connect the containers in a local environment, Docker Compose is the easiest solution but for production, the recommended approach is to use an orchestration system like <a href="https://kubernetes.io/">Kubernetes</a>.</p><h2 id="Presto"><a href="#Presto" class="headerlink" title="Presto"></a>Presto</h2><p>Following the rise of NoSQL databases, many specialized query languages exist today. This leads to huge data integration efforts and expensive ETL processes. In the Hadoop world, a number of solutions emerged to enable the usage of SQL to retrieve data, Presto being the most interesting in my opinion.</p><p>The main advantage of Presto is that it has many data connectors, such as Kafka, Cassandra, Elasticsearch, MongoDB, Postgres, etc… It is then able to infer the schema automatically and handle semi-structured data. Arrays, nested objects, multi-database joins and the regular SQL operations are all supported.</p><p><a href="https://drill.apache.org/">Apache Drill</a> is a very similar alternative to Presto, however it appears to be less popular and doesn’t support as many data sources. Performance comparisons are out of the scope for this post, but Presto is used by big players in data-intensive environments. Amazon created <a href="https://aws.amazon.com/athena">Athena</a> which is based on Presto and heavily integrated in AWS. There is another recent popular Presto fork called <a href="https://trino.io/">Trino</a>.</p><h2 id="Metabase"><a href="#Metabase" class="headerlink" title="Metabase"></a>Metabase</h2><p>There is a lot of commercial Business Intelligence software out there. Metabase stands out for being an open-source BI technology that is easy to use even for people that don’t know SQL, allowing them to explore the data and create web dashboards. Some advanced visualizations still require SQL knowledge.</p><p>Other alternatives include <a href="https://redash.io/">Redash</a> and <a href="https://superset.incubator.apache.org/">Superset</a>. From my research, these support more chart types but are less user friendly and deployment is a bit more complicated.</p><h2 id="MongoDB"><a href="#MongoDB" class="headerlink" title="MongoDB"></a>MongoDB</h2><p>MongoDB is a document-oriented database. Documents are stored as JSON objects, making it a good choice for semi-structured data with a flexible schema. It does not support SQL out of the box, which makes it harder for analysts to extract data because they have to learn another query language.</p><h2 id="Simulation"><a href="#Simulation" class="headerlink" title="Simulation"></a>Simulation</h2><p>I created 4 containers, 2 for different databases, 1 for Presto and the last for Metabase. Application data should be stored in volumes. Configuration files can either be copied and stored as part of the image or we can follow the volume approach. On Windows, named volumes must be used in some cases because of file permission issues.</p><figure class="highlight yaml"><figcaption><span>docker-compose.yml</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">&#x27;3.4&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">mongo:</span></span><br><span class="line">    <span class="attr">build:</span> <span class="string">./mongo</span></span><br><span class="line">    <span class="attr">container_name:</span> <span class="string">mongo</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">mongo-data:/data/db</span></span><br><span class="line">  <span class="attr">postgres:</span></span><br><span class="line">    <span class="attr">build:</span> <span class="string">./postgres</span></span><br><span class="line">    <span class="attr">container_name:</span> <span class="string">postgres</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">postgres-data:/var/lib/postgresql/data</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">POSTGRES_PASSWORD=secret123</span></span><br><span class="line">  <span class="attr">presto:</span></span><br><span class="line">    <span class="attr">build:</span> <span class="string">./presto</span></span><br><span class="line">    <span class="attr">container_name:</span> <span class="string">presto</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;8080:8080&quot;</span></span><br><span class="line">  <span class="attr">metabase:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">metabase/metabase:v0.33.4</span></span><br><span class="line">    <span class="attr">container_name:</span> <span class="string">metabase</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./data/metabase:/metabase-data</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;8000:3000&quot;</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">MB_DB_FILE=/metabase-data/metabase.db</span></span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"><span class="attr">volumes:</span></span><br><span class="line">  <span class="attr">mongo-data:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">mongo-data</span></span><br><span class="line">  <span class="attr">postgres-data:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">postgres-data</span></span><br></pre></td></tr></table></figure><p>I’m including here some commands that I used for testing. <a href="https://www.json-generator.com/">JSON Generator</a> is a very nice tool to create datasets. For more details about how to seed the databases and configure Presto, check the repository.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="keyword">exec</span> <span class="operator">-</span>it mongo bash</span><br><span class="line"></span><br><span class="line"># mongo</span><br><span class="line">mongo</span><br><span class="line"><span class="keyword">show</span> dbs</span><br><span class="line">use testdb</span><br><span class="line"><span class="keyword">show</span> collections</span><br><span class="line">db.orders.<span class="built_in">count</span>()</span><br><span class="line">db.users.find().limit(<span class="number">2</span>).pretty()</span><br><span class="line"></span><br><span class="line"># postgres</span><br><span class="line">su postgres</span><br><span class="line">psql</span><br><span class="line">\l  # list dbs</span><br><span class="line">\c locationdb # <span class="keyword">connect</span> <span class="keyword">to</span> locationdb</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> information_schema.schemata; # list <span class="keyword">all</span> database schemas</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> pg_tables <span class="keyword">where</span> schemaname<span class="operator">=</span><span class="string">&#x27;userlocation&#x27;</span>; # list <span class="keyword">all</span> tables <span class="keyword">from</span> schema</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> userlocation.city;</span><br><span class="line"></span><br><span class="line"># presto</span><br><span class="line">presto</span><br><span class="line"><span class="keyword">show</span> catalogs;</span><br><span class="line"><span class="keyword">show</span> schemas <span class="keyword">from</span> mongo;</span><br><span class="line"><span class="keyword">show</span> tables <span class="keyword">from</span> mongo.testdb; </span><br><span class="line"><span class="keyword">describe</span> mongo.testdb.orders;</span><br><span class="line"></span><br><span class="line"># <span class="keyword">select</span> <span class="keyword">all</span> orders, including the mongo _id</span><br><span class="line"><span class="keyword">select</span> _id, <span class="operator">*</span> <span class="keyword">from</span> mongo.testdb.orders limit <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line"># number <span class="keyword">of</span> orders</span><br><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> mongo.testdb.orders;</span><br><span class="line"></span><br><span class="line"># number <span class="keyword">of</span> items <span class="keyword">from</span> a certain <span class="keyword">order</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">cardinality</span>(items) <span class="keyword">from</span> mongo.testdb.orders <span class="keyword">where</span> id <span class="operator">=</span> <span class="string">&#x27;s1&#x27;</span>;</span><br><span class="line"></span><br><span class="line"># average number <span class="keyword">of</span> items <span class="keyword">from</span> shooping cart</span><br><span class="line"><span class="keyword">select</span> <span class="built_in">avg</span>(<span class="keyword">cardinality</span>(items)) <span class="keyword">from</span> mongo.testdb.orders;</span><br><span class="line"></span><br><span class="line"># total number <span class="keyword">of</span> items <span class="keyword">by</span> <span class="keyword">user</span></span><br><span class="line"><span class="keyword">select</span> userId, <span class="built_in">sum</span>(<span class="keyword">cardinality</span>(items)) n <span class="keyword">from</span> mongo.testdb.orders <span class="keyword">group</span> <span class="keyword">by</span> userId <span class="keyword">order</span> <span class="keyword">by</span> <span class="operator">-</span>n;</span><br><span class="line"></span><br><span class="line"># orders <span class="keyword">between</span> <span class="number">2</span> dates, <span class="keyword">for</span> <span class="keyword">some</span> reason have <span class="keyword">to</span> <span class="keyword">add</span> an <span class="keyword">OR</span> <span class="keyword">condition</span> <span class="keyword">for</span> it <span class="keyword">to</span> work correctly, maybe a bug?</span><br><span class="line"><span class="keyword">select</span> _id, id, &quot;when&quot;, <span class="keyword">year</span>(&quot;when&quot;) y, <span class="keyword">month</span>(&quot;when&quot;) m, <span class="keyword">day</span>(&quot;when&quot;) d <span class="keyword">from</span> mongo.testdb.orders <span class="keyword">where</span> _id <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">or</span> &quot;when&quot; <span class="keyword">between</span> <span class="type">timestamp</span> <span class="string">&#x27;2019-10-28&#x27;</span> <span class="keyword">and</span> <span class="type">timestamp</span> <span class="string">&#x27;2019-11-02&#x27;</span>;</span><br><span class="line"></span><br><span class="line"># unnest objects <span class="keyword">in</span> arrays</span><br><span class="line"><span class="keyword">select</span> id, userId, &quot;when&quot;, name, price, quantity <span class="keyword">from</span> mongo.testdb.orders <span class="keyword">CROSS</span> <span class="keyword">JOIN</span> <span class="built_in">UNNEST</span>(items) <span class="keyword">AS</span> t (name, price, quantity) <span class="keyword">where</span> name <span class="keyword">like</span> <span class="string">&#x27;%sic%&#x27;</span> limit <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line"># total money spent <span class="keyword">by</span> users</span><br><span class="line"><span class="keyword">select</span> <span class="built_in">sum</span>(price <span class="operator">*</span> quantity) total <span class="keyword">from</span> mongo.testdb.orders <span class="keyword">CROSS</span> <span class="keyword">JOIN</span> <span class="built_in">UNNEST</span>(items) <span class="keyword">AS</span> t (name, price, quantity);</span><br><span class="line"></span><br><span class="line"># <span class="keyword">select</span> <span class="keyword">unique</span> users that had orders <span class="keyword">and</span> <span class="keyword">join</span> info <span class="keyword">with</span> other collection <span class="keyword">and</span> <span class="keyword">table</span> <span class="keyword">in</span> other database</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">distinct</span>(userId), name, age, location, country <span class="keyword">from</span> mongo.testdb.orders o <span class="keyword">left</span> <span class="keyword">join</span> mongo.testdb.users u <span class="keyword">on</span> o.userId <span class="operator">=</span> u.id <span class="keyword">left</span> <span class="keyword">join</span> postgres.userlocation.city l <span class="keyword">on</span> u.location <span class="operator">=</span> l.city_name;</span><br><span class="line"></span><br><span class="line"># location info <span class="keyword">for</span> cities that have users <span class="operator">-</span> subqueries demo</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> postgres.userlocation.city <span class="keyword">where</span> city_name <span class="keyword">in</span> (<span class="keyword">select</span> location <span class="keyword">from</span> mongo.testdb.users);</span><br><span class="line"></span><br><span class="line"># metabase <span class="operator">-</span> number <span class="keyword">of</span> orders <span class="keyword">by</span> <span class="keyword">month</span></span><br><span class="line"><span class="keyword">SELECT</span> date_trunc(<span class="string">&#x27;month&#x27;</span>, &quot;testdb&quot;.&quot;orders&quot;.&quot;when&quot;) <span class="keyword">AS</span> &quot;when&quot;, <span class="built_in">sum</span>(price <span class="operator">*</span> quantity) <span class="keyword">AS</span> &quot;total&quot;</span><br><span class="line"><span class="keyword">FROM</span> &quot;testdb&quot;.&quot;orders&quot;</span><br><span class="line"><span class="keyword">CROSS</span> <span class="keyword">JOIN</span> <span class="built_in">UNNEST</span>(items) <span class="keyword">AS</span> t (name, price, quantity)</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> date_trunc(<span class="string">&#x27;month&#x27;</span>, &quot;testdb&quot;.&quot;orders&quot;.&quot;when&quot;)</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> date_trunc(<span class="string">&#x27;month&#x27;</span>, &quot;testdb&quot;.&quot;orders&quot;.&quot;when&quot;) <span class="keyword">ASC</span></span><br><span class="line"></span><br><span class="line"># <span class="keyword">copy</span> data <span class="keyword">from</span> named volume <span class="keyword">to</span> host</span><br><span class="line">docker run <span class="operator">-</span>v postgres<span class="operator">-</span>data:<span class="operator">/</span>volumedata <span class="comment">--name test --rm -it alpine sh</span></span><br><span class="line">docker cp test:<span class="operator">/</span>volumedata .<span class="operator">/</span>mydata</span><br><span class="line"></span><br><span class="line"># stop containers <span class="keyword">and</span> remove a volume</span><br><span class="line">docker container prune</span><br><span class="line">docker volume rm postgres<span class="operator">-</span>data</span><br></pre></td></tr></table></figure><h3 id="Metabase-setup"><a href="#Metabase-setup" class="headerlink" title="Metabase setup"></a>Metabase setup</h3><p>Unlike the other tools, on Metabase there is no way to import&#x2F;export dashboard configurations, other than copying the database it uses, which is H2 by default. For that reason I have included the data folder in the project repository.</p><img src="/2019/Query-MongoDB-using-SQL-with-Presto/metabase-setup.png" class="" title="Metabase setup"><h3 id="Presto-UI"><a href="#Presto-UI" class="headerlink" title="Presto UI"></a>Presto UI</h3><p>It is possible to see the queries that are executed by the Presto cluster. A distributed system is required to handle large amounts of data.</p><img src="/2019/Query-MongoDB-using-SQL-with-Presto/presto-ui.png" class="" title="Presto UI"><h3 id="Dashboard"><a href="#Dashboard" class="headerlink" title="Dashboard"></a>Dashboard</h3><p>This final dashboard can be obtained by navigating to <em>localhost:8000</em> after running <em>docker-compose up</em>. During the first manual setup I chose <em>test[at]example.com</em> as the username and <em>test1234</em> for the password. Ideally this should be a configuration of the image or automatically setup when the container starts.</p><img src="/2019/Query-MongoDB-using-SQL-with-Presto/dashboard-example.png" class="" title="Simulation dashboard"><h2 id="Closing-thoughts"><a href="#Closing-thoughts" class="headerlink" title="Closing thoughts"></a>Closing thoughts</h2><p>I recently had to create a web dashboard that needed data from multiple databases and these technologies allowed me to quickly build a quality prototype. Feel free to check the <a href="https://github.com/ruial/presto-simulation">repository</a>.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;With an increasing number of specialized databases, each having their own query languages, data analysts have a hard time to combine data</summary>
      
    
    
    
    
    <category term="sql" scheme="https://briefbytes.com/tags/sql/"/>
    
    <category term="mongodb" scheme="https://briefbytes.com/tags/mongodb/"/>
    
    <category term="presto" scheme="https://briefbytes.com/tags/presto/"/>
    
    <category term="metabase" scheme="https://briefbytes.com/tags/metabase/"/>
    
    <category term="docker" scheme="https://briefbytes.com/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>Game hacking on Linux</title>
    <link href="https://briefbytes.com/2019/Game-hacking-on-Linux/"/>
    <id>https://briefbytes.com/2019/Game-hacking-on-Linux/</id>
    <published>2019-07-20T15:00:00.000Z</published>
    <updated>2022-11-16T20:24:29.021Z</updated>
    
    <content type="html"><![CDATA[<p>I always liked to play computer games. They are complex pieces of software. In the past most games had cheat codes, but nowadays it’s less common. As an engineer I like to see how things work, so let’s reverse engineer an open-source game on Linux called <a href="https://www.xonotic.org/">Xonotic</a> and create a small cheat to have infinite health and ammo.</p><h2 id="How-are-game-hacks-made"><a href="#How-are-game-hacks-made" class="headerlink" title="How are game hacks made"></a>How are game hacks made</h2><p>To completely understand how cheats are made, some knowledge about how programs and memory work is valuable. Most common operating systems allow processes to read and write memory on other processes, which can be used to cheat in games. Values such as health are often stored in dynamically allocated memory. This means that when the game is restarted, the memory address that keeps the health will change. However, there is always some static base address that points to the health address, we just have to follow the pointers using static offsets.</p><h2 id="Cheat-Engine"><a href="#Cheat-Engine" class="headerlink" title="Cheat Engine"></a>Cheat Engine</h2><p>The most popular tool to hack games is <a href="https://www.cheatengine.org/">Cheat Engine</a>. It is an open-source memory scanner and debugger. As most games on PC are for Windows, that is the primary focus of the software. On Linux it uses a client-server architecture so we must download the Linux server and also the Windows client, which must be executed on <a href="https://www.winehq.org/">Wine</a>.</p><h3 id="Searching-the-health"><a href="#Searching-the-health" class="headerlink" title="Searching the health"></a>Searching the health</h3><p>The first step is to start the cheat engine server using sudo and then the client. Afterwards connect to the server on <em>File &gt; Open Process &gt; Network &gt; Connect</em> and select the game process.</p><p>Now we can search for the health. Start with 100, scan, take a bit of damage, scan again until we have few addresses. Green addresses are static and finding them so soon usually means that it is not the address we want. Let’s try the other address and “Find out what writes to this address”.</p><img src="/2019/Game-hacking-on-Linux/health-search.png" class="" title="Health search"><p>A bit of assembly knowledge is useful. We can see that the <strong>mov</strong> instruction copies the value from the <strong>ecx</strong> register to the address we found. On the line above, the <strong>ecx</strong> value is copied from the <strong>rdi</strong> register, which contains the address <strong>0x7FFCE0634490</strong>. As <strong>rax</strong> is 0, the addition and multiplication do nothing.</p><img src="/2019/Game-hacking-on-Linux/writes-to-health.png" class="" title="Writes to health"><p>Let’s add the address we found and “Find out what accesses to this address” and we can see that many instructions access this address. Since the health is being copied from here, we will have to search all of them until we find some register with the value 100 (decimal), which will be 0x64 (hexadecimal). Luckily, I didn’t have to search a lot and then I clicked the “Show disassembler” to open the Memory viewer. In here we can place some breakpoints and debug the program.</p><p>This is very insightful! The <strong>cvttss2si</strong> instruction is used to convert floats to integers. Going back a little we find the <strong>lea</strong> instruction which is copying the health from the address in <strong>edx + rax * 4</strong>. With the multiplication we get our first offset, <strong>0x30 * 0x4 &#x3D; 0xC0</strong>. The <strong>rdx</strong> is set based on <strong>rd12 + 08</strong>, which is <strong>0xBE4E988</strong>. We can verify this by adding the pointer and offset to the list of addresses and by setting the type to float.</p><img src="/2019/Game-hacking-on-Linux/memory-viewer.png" class="" title="Memory viewer"><p>Now we need to find out what accesses to this pointer. These steps of finding the base address may involve trial and error. Let’s pick the first instruction and we know from the <strong>mov</strong> instruction that the offset is <strong>rsi + 08</strong>, which equals <strong>0x18</strong>. The <strong>r14</strong> register has an address and by looking at an instruction above, it is obtained through <strong>rbx + 0x5C360</strong> and <strong>rbx</strong> is <strong>0x1E6AAA0</strong>. As this address is static, it is our base address to get to the health, by applying the correct offsets.</p><img src="/2019/Game-hacking-on-Linux/memory-viewer-pointer.png" class="" title="Memory viewer pointer"><h3 id="Pointer-scan"><a href="#Pointer-scan" class="headerlink" title="Pointer scan"></a>Pointer scan</h3><p>An alternative to this backtracking is when we find the real health address, we do a pointer scan. We see 2 different pointer paths, to pick the right one we can restart the game and see which still points to the health. The static base address with the offset <strong>0x18</strong> is the same as the one previously found but is getting calculated using the “xonotic-linux64-sdl” module address.</p><img src="/2019/Game-hacking-on-Linux/pointer-scan.png" class="" title="Pointer scan"><h3 id="Dissect-data-structures"><a href="#Dissect-data-structures" class="headerlink" title="Dissect data structures"></a>Dissect data structures</h3><p>To find the ammo, I took a quick shortcut. Usually games store the player data in a <strong>struct</strong> or a <strong>class</strong> and as such, it’s highly likely that the health and ammo are in close memory proximity. By using the <strong>Dissect data structures</strong> feature from the <em>Memory Viewer &gt; Tools &gt; Dissect data&#x2F;structures &gt; Structures &gt; Define new structure</em> we find that the ammo is just a few bytes away from the health, with the value 15.</p><img src="/2019/Game-hacking-on-Linux/dissect.png" class="" title="Dissect data structures"><h2 id="Coding"><a href="#Coding" class="headerlink" title="Coding"></a>Coding</h2><p>To read and write memory from other processes we need to call APIs that depend on the operating system. For Linux we can use <a href="http://man7.org/linux/man-pages/man2/ptrace.2.html">ptrace</a> or <a href="https://linux.die.net/man/2/process_vm_readv">process_vm_readv</a> and <a href="https://linux.die.net/man/2/process_vm_writev">process_vm_writev</a>. On Windows, the functions <a href="https://docs.microsoft.com/en-us/windows/win32/api/memoryapi/nf-memoryapi-readprocessmemory">ReadProcessMemory</a> and <a href="https://docs.microsoft.com/en-us/windows/win32/api/memoryapi/nf-memoryapi-writeprocessmemory">WriteProcessMemory</a> are available. Alternatively, a module (.so&#x2F;.dll) can be injected into the game to avoid using these APIs and enable direct memory access. The best language for these low level things is C or C++.</p><figure class="highlight c"><figcaption><span>Read and write memory helpers</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">bool</span> <span class="title function_">read_addr</span><span class="params">(<span class="type">pid_t</span> pid, <span class="type">unsigned</span> <span class="type">long</span> addr, <span class="type">void</span> *buffer, <span class="type">size_t</span> size)</span> &#123;</span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">iovec</span> <span class="title">local</span>[1];</span></span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">iovec</span> <span class="title">remote</span>[1];</span></span><br><span class="line"></span><br><span class="line">  local[<span class="number">0</span>].iov_base = buffer;</span><br><span class="line">  local[<span class="number">0</span>].iov_len = size;</span><br><span class="line">  remote[<span class="number">0</span>].iov_base = (<span class="type">void</span> *)addr;</span><br><span class="line">  remote[<span class="number">0</span>].iov_len = size;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> process_vm_readv(pid, local, <span class="number">1</span>, remote, <span class="number">1</span>, <span class="number">0</span>) == size;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">bool</span> <span class="title function_">write_addr</span><span class="params">(<span class="type">pid_t</span> pid, <span class="type">unsigned</span> <span class="type">long</span> addr, <span class="type">void</span> *buffer, <span class="type">size_t</span> size)</span> &#123;</span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">iovec</span> <span class="title">local</span>[1];</span></span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">iovec</span> <span class="title">remote</span>[1];</span></span><br><span class="line"></span><br><span class="line">  local[<span class="number">0</span>].iov_base = buffer;</span><br><span class="line">  local[<span class="number">0</span>].iov_len = size;</span><br><span class="line">  remote[<span class="number">0</span>].iov_base = (<span class="type">void</span> *)addr;</span><br><span class="line">  remote[<span class="number">0</span>].iov_len = size;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> process_vm_writev(pid, local, <span class="number">1</span>, remote, <span class="number">1</span>, <span class="number">0</span>) == size;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Using system calls is an expensive operation. As such, it is better to create a <strong>struct</strong> to hold the player information and read one bigger chunk of memory at once, than many small chunks. As we are getting to the dynamic player structure address by reading pointers through a static base address and offsets, the cheat will always work when the game is restarted, however these offsets may change when the game is updated. There are alternatives to get to the dynamic address that may resist game updates which are based on signature&#x2F;AOB(array of bytes) scans.</p><figure class="highlight c"><figcaption><span>Memory structures, read struct and write health</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> PLAYER_OFFSET_1 0x1AC6E00</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> PLAYER_OFFSET_2 0x18</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> HEALTH_OFFSET 0xC0</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> AMMO_OFFSET 0xD8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">  <span class="type">bool</span> health;</span><br><span class="line">  <span class="type">bool</span> ammo;</span><br><span class="line">&#125; Options;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">  <span class="type">pid_t</span> pid;</span><br><span class="line">  <span class="type">unsigned</span> <span class="type">long</span> module;</span><br><span class="line">  Options options;</span><br><span class="line">&#125; Game;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">  <span class="type">char</span> _1[<span class="number">0xC0</span>];</span><br><span class="line">  <span class="type">float</span> health;  <span class="comment">// 0xC0</span></span><br><span class="line">  <span class="type">char</span> _2[<span class="number">0x14</span>]; <span class="comment">// 0xD8 - 0xC0 - 0x4(sizeof float)</span></span><br><span class="line">  <span class="type">float</span> ammo;    <span class="comment">// 0xD8</span></span><br><span class="line">  <span class="type">unsigned</span> <span class="type">long</span> address;</span><br><span class="line">&#125; MyPlayer;</span><br><span class="line"></span><br><span class="line"><span class="type">bool</span> <span class="title function_">readMyPlayer</span><span class="params">(Game game, MyPlayer *myPlayer)</span> &#123;</span><br><span class="line">  <span class="type">unsigned</span> <span class="type">long</span> ptr;</span><br><span class="line">  <span class="keyword">if</span> (read_addr(game.pid, game.module + PLAYER_OFFSET_1, &amp;ptr, <span class="keyword">sizeof</span>(ptr))) &#123;</span><br><span class="line">    <span class="keyword">if</span> (read_addr(game.pid, ptr + PLAYER_OFFSET_2, &amp;ptr, <span class="keyword">sizeof</span>(ptr))) &#123;</span><br><span class="line">      myPlayer-&gt;address = ptr;</span><br><span class="line">      <span class="keyword">return</span> read_addr(game.pid, ptr, myPlayer,</span><br><span class="line">                       <span class="keyword">sizeof</span>(MyPlayer) - <span class="keyword">sizeof</span>(myPlayer-&gt;address));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">writeHealth</span><span class="params">(Game game, MyPlayer myPlayer, <span class="type">float</span> value)</span> &#123;</span><br><span class="line">  write_addr(game.pid, myPlayer.address + HEALTH_OFFSET, &amp;value, <span class="keyword">sizeof</span>(value));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Cheats are usually running in some infinite loop until they detect the game is not available. Sleep commands avoid hitting unnecessary 100% CPU usage. Global keyboard input detection to toggle features is helpful (code is OS dependent). An alternative to writing the health constantly, would be to patch the code that decreases it, like replacing a <strong>mov</strong> with <strong>nop</strong> instructions, which could be done by writing the correct bytes at the correct address.</p><figure class="highlight c"><figcaption><span>Main loop</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> <span class="title function_">open_game</span><span class="params">(Game *game)</span> &#123;</span><br><span class="line">  <span class="keyword">while</span> ((game-&gt;pid = find_pid(<span class="string">&quot;xonotic-linux64-sdl&quot;</span>)) == <span class="number">0</span>) &#123;</span><br><span class="line">    sleep_ms(<span class="number">1000</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">while</span> ((game-&gt;module = module_addr(game-&gt;pid, <span class="string">&quot;xonotic-linux64-sdl&quot;</span>)) == <span class="number">0</span>) &#123;</span><br><span class="line">    sleep_ms(<span class="number">1000</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span> &#123;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;waiting for xonotic...\n&quot;</span>);</span><br><span class="line">  Game game;</span><br><span class="line">  open_game(&amp;game);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;pid: %d\n&quot;</span>, game.pid);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;xonotic-linux64-sdl module: %lx\n&quot;</span>, game.module);</span><br><span class="line"></span><br><span class="line">  MyPlayer myPlayer;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (readMyPlayer(game, &amp;myPlayer)) &#123;</span><br><span class="line">      manage_input(&amp;game);</span><br><span class="line">      <span class="keyword">if</span> (game.options.health) &#123;</span><br><span class="line">        writeHealth(game, myPlayer, <span class="number">150</span>);</span><br><span class="line">      &#125;</span><br><span class="line">      printPlayer(myPlayer);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">&quot;not in arena\n&quot;</span>);</span><br><span class="line">      sleep_ms(<span class="number">1000</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    sleep_ms(<span class="number">50</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Keep in mind that this hack works for single player only. Server side software should keep their own health value for each player and as such we can’t change it, any local change will be visual only.</p><h2 id="Closing-thoughts"><a href="#Closing-thoughts" class="headerlink" title="Closing thoughts"></a>Closing thoughts</h2><p>Reverse engineering is hard. I admire the researchers who have to analyze software&#x2F;malware in similar ways. Making complex cheats is also extremely time consuming. For example, we can draw enemies through walls or even automatically aim and shoot against them by reading their coordinates and applying some game&#x2F;engine dependent math, but a lot of study is required.</p><p>The full source code is available on <a href="https://github.com/ruial/linux-game-hack-example">GitHub</a> and was tested on Ubuntu 16.04 and Xonotic v0.8.2.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;I always liked to play computer games. They are complex pieces of software. In the past most games had cheat codes, but nowadays it’s les</summary>
      
    
    
    
    
    <category term="hacking" scheme="https://briefbytes.com/tags/hacking/"/>
    
    <category term="linux" scheme="https://briefbytes.com/tags/linux/"/>
    
    <category term="c" scheme="https://briefbytes.com/tags/c/"/>
    
  </entry>
  
  <entry>
    <title>Debugging mobile apps with an HTTPS proxy</title>
    <link href="https://briefbytes.com/2019/Debugging-mobile-apps-with-an-https-proxy/"/>
    <id>https://briefbytes.com/2019/Debugging-mobile-apps-with-an-https-proxy/</id>
    <published>2019-05-27T22:00:00.000Z</published>
    <updated>2022-11-16T20:24:29.017Z</updated>
    
    <content type="html"><![CDATA[<p>As a part of my previous job, I occasionally had to intercept HTTPS requests made from a mobile application. This is required to troubleshoot some issues and guarantee that the correct data is being sent. I will explain how I do it and talk about analytics while I briefly analyze a popular app.</p><h2 id="Analytics"><a href="#Analytics" class="headerlink" title="Analytics"></a>Analytics</h2><p>How much time users spend on the app? How many times do they return per month? Does performance have a significant impact on other metrics? Which button is clicked the most from our A&#x2F;B test? These are questions that can be answered by analytics and help in making data-driven decisions which previously relied on intuition.</p><p>As storage and computing resources are getting cheaper, data collection and processing increases. Many apps track all the relevant events and aggregate this data for further analysis.</p><p>Let’s take a look at the Airbnb app. They are hitting 2 different tracking endpoints. Events are tracked regarding navigation, performance, searches, app lifecycle, and contain information about the device such as the model, screen size, network type, language, etc… On the app, these events are sent in batches to reduce bandwidth usage, which is not the case on their website as scrolling through the main page will trigger dozens of tracking requests, as we can check in the dev tools. An example of the tracked events of the app can be found in the image bellow.</p><img src="/2019/Debugging-mobile-apps-with-an-https-proxy/airbnb-tracking.png" class="" title="Airbnb Tracking"><p>We can observe an object with a key called “advertising-ID” with the value containing several zeros. One feature not known by many people and enabled by default in Android and iOS is the advertising ID. It is a random unique identifier that ad companies can use to track users, similar to a tracking cookie on the web. In the phone settings we can request a new advertising ID or even disable it completely, forcing apps to rely on other data to track users (IP address, device fingerprinting, generated identifiers).</p><p>To disable the advertising ID follow these steps:</p><ul><li>Android<ul><li><em>Settings &gt; Google &gt; Ads &gt; Opt out of Ads Personalization</em></li></ul></li><li>iOS<ul><li><em>Settings &gt; Privacy &gt; Advertising &gt; Limit Ad tracking</em></li></ul></li></ul><h2 id="mitmproxy"><a href="#mitmproxy" class="headerlink" title="mitmproxy"></a>mitmproxy</h2><p>One useful open-source HTTPS proxy is <a href="https://mitmproxy.org/">mitmproxy</a>. It allows to intercept and modify requests through the terminal or web interface. Another popular free alternative with a focus on web security is <a href="https://portswigger.net/burp">Burp Suite</a>. This way we can reverse engineer the APIs used by apps.</p><p>The quickest way to start mitmproxy is with Docker:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --<span class="built_in">rm</span> -it -v ~/.mitmproxy:/home/mitmproxy/.mitmproxy -p 8080:8080 -p 127.0.0.1:8081:8081 mitmproxy/mitmproxy mitmweb --web-iface 0.0.0.0</span><br></pre></td></tr></table></figure><p>Now we can access the UI at <em>localhost:8081</em>. To set the proxy on both iOS and Android devices, go to “<em>Settings &gt; Wi-Fi</em>“, tap the connected network and enter the computer’s IP address as the server&#x2F;hostname and <em>8080</em> as the port. In case the connection is failing, the computer’s firewall may be the issue.</p><p>If the root certificate was not previously installed, visit <em>mitm.it</em> on your browser and click the image of your operating system to install the certificates. For Android, a dialog will appear to choose a certificate name and select “VPN and apps” as the “Credential use”. For iOS go to “<em>Settings &gt; General &gt; Profiles</em>“, tap on the created profile to install the certificate and then enable it in “<em>Settings &gt; General &gt; About &gt; Certificate Trust Settings</em>“.</p><p>Nowadays most popular apps employ a security technique called <a href="https://www.owasp.org/index.php/Certificate_and_Public_Key_Pinning">Certificate Pinning</a>. If the certificate is not trusted by the app, connections are dropped. To perform man in the middle attacks we now have to tamper with the app to disable certificate pinning using toolkits like <a href="https://github.com/sensepost/objection">objection</a> and <a href="https://www.frida.re/">Frida</a>.</p><h2 id="Closing-thoughts"><a href="#Closing-thoughts" class="headerlink" title="Closing thoughts"></a>Closing thoughts</h2><p>mitmproxy is a very useful tool. Addons like this one can be coded in Python:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mitmproxy <span class="keyword">import</span> http, ctx</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> randint</span><br><span class="line"></span><br><span class="line"><span class="comment"># simulate failure in 25% of tracking requests</span></span><br><span class="line"><span class="comment"># Usage (has live reload): mitmweb -s intercept.py</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">request</span>(<span class="params">flow: http.HTTPFlow</span>):</span><br><span class="line">  url = flow.request.url</span><br><span class="line">  <span class="keyword">if</span> <span class="string">&#x27;tracking&#x27;</span> <span class="keyword">in</span> url <span class="keyword">and</span> randint(<span class="number">1</span>, <span class="number">100</span>) &lt; <span class="number">25</span>:</span><br><span class="line">    ctx.log.warn(<span class="string">&#x27;Fail &#x27;</span> + url)</span><br><span class="line">    flow.response = http.HTTPResponse.make(<span class="number">500</span>, <span class="string">&#x27;&#x27;</span>, &#123;&#125;)</span><br></pre></td></tr></table></figure><p>I think that analytics is very important to figure out how users are interacting with the software. The data collected can be analyzed to provide a better service by identifying issues and trends, however care should be taken to respect user’s privacy and comply with <em>GDPR</em>.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;As a part of my previous job, I occasionally had to intercept HTTPS requests made from a mobile application. This is required to troubles</summary>
      
    
    
    
    
    <category term="analytics" scheme="https://briefbytes.com/tags/analytics/"/>
    
    <category term="android" scheme="https://briefbytes.com/tags/android/"/>
    
    <category term="ios" scheme="https://briefbytes.com/tags/ios/"/>
    
  </entry>
  
</feed>
